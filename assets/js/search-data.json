{
  
    
        "post0": {
            "title": "Variational Inference - ELBO",
            "content": "We don’t know the real posterior so we are going to choose a distribution $Q( theta)$ from a family of distributions $Q^$ that are easy to work with and parameterized by $ theta$. The approximate distribution should be *as close as possible to the true posterior. This closeness is measured using KL-Divergence. If we have the joint $p(x, z)$ where $x$ is some observed data, the goal is to perform inference: given what we have observed, what can we infer about the latent states?, i.e , we want the posterior. . Recall Bayes theorem: . p(z∣x)=p(x∣z)p(z)p(x)p(z | x) = frac{p(x|z)p(z)}{p(x)}p(z∣x)=p(x)p(x∣z)p(z)​ . The problem is the marginal $p(x = D)$ as this could require a hundred, thousand, . . .dimensional integral: . p(x)=∫z0,...,∫zD−1p(x,z)dz0,...,dzD−1p(x) = int_{z_0},..., int_{z_{D-1}}p(x, z)dz_0,...,d_{z_D{-1}}p(x)=∫z0​​,…,∫zD−1​​p(x,z)dz0​,…,dzD​−1​ . If we want the full posterior and can’t compute the marginal, then what’s the solution? Surrogate posterior. We want to approximate the true posterior using some known distribution: . q(z)≈p(z∣X=D)q(z) approx p(z|X=D)q(z)≈p(z∣X=D) . where $ approx$ can mean you want the approximated posterior to be “as good as possible”. Using variational inference, the objective is to minimize the distance between the surrogate $q(z)$ and the true posterior $p(x)$ using KL-Divergence: . q∗(z)=argminq(z)∈Q(KL(q(z)∣∣p(z∣x=D)))q^*(z) = argmin_{q(z) in Q} (KL(q(z) || p(z|x=D)))q∗(z)=argminq(z)∈Q​(KL(q(z)∣∣p(z∣x=D))) . where $Q$ is a more “simple” distribution. We can restate the KL-divergence as the expectation: . KL(q(z)∣∣p(z∣D))=Ez∼q(z)[logq(z)p(z∣D)]KL(q(z) || p(z|D)) = mathbb{E_{z sim q(z)}}[log frac{q(z)}{p(z|D)}]KL(q(z)∣∣p(z∣D))=Ez∼q(z)​[logp(z∣D)q(z)​] . which, taking the expectation over $z$, is equivalent to integration: . ∫z0,...,∫zD−1q(z)logq(z)p(z∣D)dz0,...,dzD−1 int_{z_0}, . . ., int_{z_{D-1}}q(z)log frac{q(z)}{p(z|D)}d_{z_0},...,d_{z_{D-1}}∫z0​​,…,∫zD−1​​q(z)logp(z∣D)q(z)​dz0​​,…,dzD−1​​ . But, sadly we don’t have $p(z vert D)$ as this is the posterior! We only have the joint. Solution? Recall our KL-divergence: . KL(q(z)∣∣p(z∣D))KL(q(z) || p(z|D))KL(q(z)∣∣p(z∣D)) . We can rearrange the terms inside the $log$ so that we can actually compute something: . ∫zq(z)log(q(z)p(D)p(z,D))dz int_{z}q(z)log( frac{q(z)p(D)}{p(z, D)})dz∫z​q(z)log(p(z,D)q(z)p(D)​)dz . We only have the joint. Not the posterior; nor the marginal. We know from Bayes rule that we can express the posterior in terms of the joint $p(z, D)$ divided by the marginal $p(x=D)$: . p(z∣D)=p(Z,D)p(D)p(z|D) = frac{p(Z, D)}{p(D)}p(z∣D)=p(D)p(Z,D)​ . We plug this inside of the $log$: . ∫zq(z)log(q(z)p(D)p(z,D))dz int_{z}q(z)log( frac{q(z)p(D)}{p(z, D)})dz∫z​q(z)log(p(z,D)q(z)p(D)​)dz . However, the problem now is that we have reformulated our problem into another quantity that we don’t have, i.e., the marginal $p(D)$. But we can put the quantity that we don’t have outside of the $log$ to form two separate integrals. . ∫zq(z)log(q(z)p(z,D))dz+∫zq(z)log(p(D)dz int_z q(z)log( frac{q(z)}{p(z, D)})dz + int_zq(z)log(p(D)dz∫z​q(z)log(p(z,D)q(z)​)dz+∫z​q(z)log(p(D)dz . This is a valid rearrangement because of the properties of logarithms. In this case, the numerator is a product, so this turns into a sum of the second integral. What do we see in these two terms? We see an expectation over the quantity $ frac{q(z)}{p(z, D)}$ and another expectation over $p(D)$. Rewriting in terms of expectation: . Ez∼q(z)[log(q(z)p(z,D))]+Ez∼q(z)[log(p(D))] mathbb{E_{z{ sim q(z)}}}[log( frac{q(z)}{p(z, D)})] + mathbb{E_{z sim q(z)}}[log(p(D))]Ez∼q(z)​[log(p(z,D)q(z)​)]+Ez∼q(z)​[log(p(D))] . The right term contains information we know—the functional form of the surrogate $q(z)$ and the joint $p(z, D)$ (in the form of a directed graphical model). We still don’t have access to $p(D)$ on the right side, but this is a constant quantity. The expectation of a quantity that does not contain $z$ is just whatever the expectation was taken over. Because of this, we can again rearrange: . −Ez∼q(z)[logp(z,D)q(z)]+log(p(D))- mathbb{E_{z sim q(z)}}[log frac{p(z, D)}{q(z)}]+log (p(D))−Ez∼q(z)​[logq(z)p(z,D)​]+log(p(D)) . The minus sign is a result of the “swapping” of the numerator and denominator and is required to make it a valid change. Looking at this, the left side is a function dependent on $q$. In shorthand form, we can call this $ mathcal{L(q)}$. Our KL-divergence is: . KL=−L(q)+log(p(D))⏟evidenceKL = mathcal{-L(q)} + underbrace{log(p(D))}_ textrm{evidence}KL=−L(q)+evidence . log(p(D))​​ . where $p(D)$ is a value between $[0, 1]$ and this value is called the evidence which is the log probability of the data. If we apply the $log$ to something between $[0, 1]$ then this value will be negative. This value is also constant since we have observed the dataset and thus does not change. . $KL$ is the distance (between the posterior and the surrogate) so it must be something positive. If the $KL$ is positive and the evidence is negative, then in order to fulfill this equation, $ mathcal{L}$ must also be negative (negative times a negative is a positive). The $ mathcal{L}$ should be smaller than the evidence, and thus it is called the lower bound of the evidence $ rightarrow$ Evidence Lower Bound (ELBO). . Again, ELBO is defined as: $ mathcal{L} = mathbb{E_{z sim q(z)}}[log( frac{p(z, D)}{q(z)})]$ and is important to note that the ELBO is equal to the evidence if and only if the KL-divergence between the surrogate and the true posterior is $0$: . L(q)=log(p(D)) i.f.f. KL(q(z)∣∣p(z∣D))=0 mathcal{L(q)} = log(p(D)) textrm{ i.f.f. } KL(q(z)||p(z|D))=0L(q)=log(p(D)) i.f.f. KL(q(z)∣∣p(z∣D))=0 .",
            "url": "https://gstechschulte.github.io/cached-projects/probability/optimization/2022/06/23/ELBO.html",
            "relUrl": "/probability/optimization/2022/06/23/ELBO.html",
            "date": " • Jun 23, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "One is a Crowd",
            "content": "In the book, Noise: A Flaw in Human Judgement, the authors point out a study conducted by Edward Val and Harold Pashler in which the subjects answer a question, and then subsequently, are asked to answer the same question again. The subjects did not know they would have to answer the question twice. Val and Pashler’s hypothesis was that the average of the two answers would be more accurate than either of the two answers on their own. The researchers who conducted the study drew inspiration from the wisdom of the crowds effect—averaging independent judgements of different people generally improves accuracy. . We do not always produce identical judgements when faced with the same facts on different occasions. .",
            "url": "https://gstechschulte.github.io/cached-projects/probability/reading/2022/06/21/Judgements-Probability.html",
            "relUrl": "/probability/reading/2022/06/21/Judgements-Probability.html",
            "date": " • Jun 21, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Bayesian Modeling and Computation in Python",
            "content": "import pandas as pd import matplotlib.pyplot as plt import numpy as np import arviz as az import torch import pyro import pyro.distributions as dist from pyro.infer import Predictive, NUTS, MCMC from pyro.infer.autoguide import AutoLaplaceApproximation from pyro.infer import SVI, Trace_ELBO import pyro.optim as optim from palmerpenguins import load_penguins . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html from .autonotebook import tqdm as notebook_tqdm . penguins = load_penguins() penguins.dropna(how=&#39;any&#39;, axis=0, inplace=True) . penguins . species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year . 0 Adelie | Torgersen | 39.1 | 18.7 | 181.0 | 3750.0 | male | 2007 | . 1 Adelie | Torgersen | 39.5 | 17.4 | 186.0 | 3800.0 | female | 2007 | . 2 Adelie | Torgersen | 40.3 | 18.0 | 195.0 | 3250.0 | female | 2007 | . 4 Adelie | Torgersen | 36.7 | 19.3 | 193.0 | 3450.0 | female | 2007 | . 5 Adelie | Torgersen | 39.3 | 20.6 | 190.0 | 3650.0 | male | 2007 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 339 Chinstrap | Dream | 55.8 | 19.8 | 207.0 | 4000.0 | male | 2009 | . 340 Chinstrap | Dream | 43.5 | 18.1 | 202.0 | 3400.0 | female | 2009 | . 341 Chinstrap | Dream | 49.6 | 18.2 | 193.0 | 3775.0 | male | 2009 | . 342 Chinstrap | Dream | 50.8 | 19.0 | 210.0 | 4100.0 | male | 2009 | . 343 Chinstrap | Dream | 50.2 | 18.7 | 198.0 | 3775.0 | female | 2009 | . 333 rows × 8 columns . Pyro . adelie_mask = (penguins[&#39;species&#39;] == &#39;Adelie&#39;) adelie_mass_obs = torch.from_numpy(penguins.loc[adelie_mask, &#39;body_mass_g&#39;].values) . Code 3.3 . def model_prior(obs=None): # priors over params. sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(100., 2000.)) mu = pyro.sample(&#39;mu&#39;, dist.Normal(4000, 3000)) mass = pyro.sample(&#39;mass&#39;, dist.Normal(mu, sigma), obs=obs) . prior_samples = Predictive( model_prior, {}, num_samples=1000, return_sites=[&#39;sigma&#39;, &#39;mu&#39;, &#39;mass&#39;])(adelie_mass_obs) . az.plot_density(data=prior_samples[&#39;mass&#39;].numpy()) . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/data/base.py:220: UserWarning: More chains (1000) than draws (146). Passed array should have shape (chains, draws, *shape) warnings.warn( . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fe40d891d00&gt;]], dtype=object) . az.plot_density(data=prior_samples[&#39;mu&#39;].numpy()) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fe40d938490&gt;]], dtype=object) . az.plot_density(data=prior_samples[&#39;sigma&#39;].numpy()) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fe40d986c10&gt;]], dtype=object) . def model(obs=None): # priors over params. sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(100., 2000.)) mu = pyro.sample(&#39;mu&#39;, dist.Normal(4000, 3000)) mass = pyro.sample(&#39;mass&#39;, dist.Normal(mu, sigma), obs=obs) return mass . kernel = NUTS(model) mcmc = MCMC(kernel, num_samples=2000, warmup_steps=1000) mcmc.run(obs=adelie_mass_obs) . Warmup: 0%| | 0/3000 [15:32, ?it/s] Sample: 100%|██████████| 3000/3000 [00:38, 78.75it/s, step size=9.22e-01, acc. prob=0.914] . mcmc.summary() . mean std median 5.0% 95.0% n_eff r_hat mu 3706.26 35.61 3707.46 3644.90 3761.97 1820.74 1.00 sigma 433.33 23.06 432.48 395.66 471.65 1112.60 1.00 Number of divergences: 0 . samples_1 = mcmc.get_samples() . samples_1 . {&#39;mu&#39;: tensor([3761.4877, 3744.2118, 3708.0032, ..., 3683.6709, 3713.1172, 3619.1878], dtype=torch.float64), &#39;sigma&#39;: tensor([417.8094, 422.2480, 436.3104, ..., 455.0801, 415.7282, 437.3125], dtype=torch.float64)} . Section 3.4.2 . species_filter = penguins[&#39;species&#39;].isin([&#39;Adelie&#39;, &#39;Chinstrap&#39;]) penguins = penguins[species_filter] . penguins[&#39;bill_length_mm&#39;] = (penguins[&#39;bill_length_mm&#39;] - penguins[&#39;bill_length_mm&#39;].mean()) / (penguins[&#39;bill_length_mm&#39;].std()) . penguins[&#39;sex_code&#39;] = pd.Categorical(penguins[&#39;sex&#39;]).codes penguins[&#39;species_code&#39;] = pd.Categorical(penguins[&#39;species&#39;]).codes . penguins[&#39;species_code&#39;].value_counts() . 0 146 1 68 Name: species_code, dtype: int64 . data = torch.tensor( penguins[&#39;bill_length_mm&#39;].values, dtype=torch.float ) outcome = torch.tensor( penguins[&#39;species_code&#39;].values, dtype=torch.float ) . def model(bill_length, outcome=None): beta_0 = pyro.sample(&#39;beta_0&#39;, dist.Normal(0., 10.)) beta_1 = pyro.sample(&#39;beta_1&#39;, dist.Normal(0., 10.)) mu = beta_0 + bill_length * beta_1 theta = pyro.deterministic(&#39;mu&#39;, torch.sigmoid(mu)) db = pyro.deterministic(&#39;db&#39;, -beta_0 / beta_1) probs_species = pyro.sample(&#39;probs_species&#39;, dist.Bernoulli(theta), obs=outcome) . kernel = NUTS(model) mcmc = MCMC( kernel, warmup_steps=1000, num_samples=3000 ) mcmc.run(data, outcome) . Sample: 100%|██████████| 4000/4000 [00:23, 167.44it/s, step size=6.99e-01, acc. prob=0.910] . mcmc.summary() . mean std median 5.0% 95.0% n_eff r_hat beta_0 -2.25 0.48 -2.23 -3.02 -1.47 1116.03 1.00 beta_1 6.03 1.01 5.96 4.45 7.62 990.57 1.00 Number of divergences: 0 . coef = mcmc.get_samples(num_samples=5000) . coef . {&#39;beta_0&#39;: tensor([-2.5377, -2.2180, -1.7581, ..., -2.7042, -2.6922, -2.3471]), &#39;beta_1&#39;: tensor([5.9295, 5.4665, 4.9583, ..., 6.0508, 7.7903, 5.4161])} . az.from_pyro( posterior=mcmc ) . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/data/io_pyro.py:157: UserWarning: Could not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False warnings.warn( . arviz.InferenceData posterior . . . . . . . &lt;xarray.Dataset&gt; Dimensions: (chain: 1, draw: 3000) Coordinates: * chain (chain) int64 0 * draw (draw) int64 0 1 2 3 4 5 6 7 ... 2993 2994 2995 2996 2997 2998 2999 Data variables: beta_0 (chain, draw) float32 -2.387 -2.38 -2.005 ... -2.52 -2.347 -2.351 beta_1 (chain, draw) float32 5.938 5.825 4.99 4.757 ... 6.214 5.827 6.157 Attributes: created_at: 2022-06-14T12:24:15.240916 arviz_version: 0.12.1 inference_library: pyro inference_library_version: 1.8.0+0ec1e87 . xarray.DatasetDimensions:chain: 1 | draw: 3000 | . | Coordinates: (2)chain(chain)int640array([0]) . | draw(draw)int640 1 2 3 4 ... 2996 2997 2998 2999array([ 0, 1, 2, ..., 2997, 2998, 2999]) . | . | Data variables: (2)beta_0(chain, draw)float32-2.387 -2.38 ... -2.347 -2.351array([[-2.3870943, -2.3796346, -2.0052505, ..., -2.5196037, -2.3474796, -2.350969 ]], dtype=float32) . | beta_1(chain, draw)float325.938 5.825 4.99 ... 5.827 6.157array([[5.938016 , 5.8249936, 4.990339 , ..., 6.2137275, 5.8271985, 6.157179 ]], dtype=float32) . | . | Attributes: (4)created_at :2022-06-14T12:24:15.240916arviz_version :0.12.1inference_library :pyroinference_library_version :1.8.0+0ec1e87 | . . | sample_stats . . . . . . . &lt;xarray.Dataset&gt; Dimensions: (chain: 1, draw: 3000) Coordinates: * chain (chain) int64 0 * draw (draw) int64 0 1 2 3 4 5 6 ... 2993 2994 2995 2996 2997 2998 2999 Data variables: diverging (chain, draw) bool False False False False ... False False False Attributes: created_at: 2022-06-14T12:24:15.328276 arviz_version: 0.12.1 inference_library: pyro inference_library_version: 1.8.0+0ec1e87 . xarray.DatasetDimensions:chain: 1 | draw: 3000 | . | Coordinates: (2)chain(chain)int640array([0]) . | draw(draw)int640 1 2 3 4 ... 2996 2997 2998 2999array([ 0, 1, 2, ..., 2997, 2998, 2999]) . | . | Data variables: (1)diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False]]) . | . | Attributes: (4)created_at :2022-06-14T12:24:15.328276arviz_version :0.12.1inference_library :pyroinference_library_version :1.8.0+0ec1e87 | . . | observed_data . . . . . . . &lt;xarray.Dataset&gt; Dimensions: (mu_dim_0: 214, db_dim_0: 1, probs_species_dim_0: 214) Coordinates: * mu_dim_0 (mu_dim_0) int64 0 1 2 3 4 5 ... 209 210 211 212 213 * db_dim_0 (db_dim_0) int64 0 * probs_species_dim_0 (probs_species_dim_0) int64 0 1 2 3 ... 210 211 212 213 Data variables: mu (mu_dim_0) float32 0.002819 0.00125 ... 4.273e-13 db (db_dim_0) float32 -1.054 probs_species (probs_species_dim_0) float32 0.0 0.0 0.0 ... 1.0 1.0 Attributes: created_at: 2022-06-14T12:24:15.418491 arviz_version: 0.12.1 inference_library: pyro inference_library_version: 1.8.0+0ec1e87 . xarray.DatasetDimensions:mu_dim_0: 214 | db_dim_0: 1 | probs_species_dim_0: 214 | . | Coordinates: (3)mu_dim_0(mu_dim_0)int640 1 2 3 4 5 ... 209 210 211 212 213array([ 0, 1, 2, ..., 211, 212, 213]) . | db_dim_0(db_dim_0)int640array([0]) . | probs_species_dim_0(probs_species_dim_0)int640 1 2 3 4 5 ... 209 210 211 212 213array([ 0, 1, 2, ..., 211, 212, 213]) . | . | Data variables: (3)mu(mu_dim_0)float320.002819 0.00125 ... 4.273e-13array([2.81905173e-03, 1.24994444e-03, 2.45209027e-04, 2.73033440e-01, 1.87743700e-03, 4.23092628e-03, 2.30067386e-03, 4.80652707e-05, 7.76804518e-03, 9.64387715e-01, 3.15276086e-01, 6.34541595e-03, 2.77470895e-06, 9.76019442e-01, 2.22144081e-09, 3.84129658e-02, 4.66872938e-02, 6.57114625e-01, 1.73772369e-02, 5.18196402e-03, 8.66778910e-01, 1.33095731e-04, 1.63164164e-04, 3.15565690e-02, 1.63164164e-04, 1.24994444e-03, 1.19425789e-01, 1.24994444e-03, 7.22384939e-05, 4.08992767e-01, 2.30067386e-03, 5.18196402e-03, 5.11255348e-06, 5.66388704e-02, 6.78762095e-04, 3.60810101e-01, 8.85595291e-05, 6.09864593e-01, 1.06570546e-07, 1.69321761e-01, 1.01980451e-03, 4.80652707e-05, 6.09864593e-01, 4.17027422e-06, 1.01980451e-03, 3.68494249e-04, 9.23007548e-01, 7.68395057e-06, 9.70759451e-01, 2.60867200e-05, 3.45382537e-03, 1.33095731e-04, 3.60810101e-01, 5.66388704e-02, 7.42288947e-01, 3.19809260e-05, 5.66388704e-02, 4.80652707e-05, 4.08992767e-01, 1.73570497e-05, 8.12347591e-01, 4.80652707e-05, 6.57114625e-01, 1.15486346e-05, 9.96087670e-01, 8.32002959e-04, 1.01980451e-03, 3.33873618e-09, 8.12347591e-01, 1.50590131e-06, 7.22384939e-05, 1.19425789e-01, 5.09825408e-01, 6.26774727e-06, 9.64387715e-01, 1.22835240e-06, 2.73033440e-01, 9.07224715e-01, 9.96071473e-02, 3.19809260e-05, ... 9.99773324e-01, 1.08567707e-04, 9.96071473e-02, 3.45382537e-03, 2.30067386e-03, 3.15276086e-01, 6.09864593e-01, 3.84129658e-02, 6.09864593e-01, 2.12788382e-05, 8.02168276e-10, 6.42215447e-13, 4.54485862e-14, 7.54183205e-09, 2.62352679e-15, 1.13350911e-08, 1.81200854e-09, 4.54485862e-14, 2.22144081e-09, 4.54485862e-14, 6.54321153e-10, 2.01198531e-14, 2.89665153e-10, 1.09195252e-14, 2.72338108e-09, 2.31905749e-13, 3.48545005e-13, 5.36750908e-20, 9.83418347e-10, 3.27695922e-12, 3.40165957e-06, 1.36391870e-11, 6.66655865e-07, 1.89163707e-13, 5.33724731e-10, 1.09195252e-14, 2.31905749e-13, 1.77848415e-12, 9.83418347e-10, 2.13999444e-15, 7.22384939e-05, 1.23531986e-16, 2.77470895e-06, 8.37417904e-14, 1.18331896e-12, 1.04598878e-10, 8.53203203e-11, 1.09195252e-14, 3.55115048e-10, 5.14157310e-16, 4.92514753e-12, 1.47804113e-09, 1.02663470e-13, 6.15181328e-09, 1.02663470e-13, 1.25860544e-13, 5.23850169e-13, 4.92514753e-12, 3.02393879e-14, 9.65223994e-13, 3.08094002e-11, 3.70720571e-14, 4.09313250e-09, 1.54299371e-13, 2.77470895e-06, 7.26532007e-15, 1.13350911e-08, 2.67299030e-12, 4.27300475e-13, 5.01798603e-09, 1.33868204e-14, 4.35355085e-10, 4.09313250e-09, 4.74459136e-18, 3.61809526e-07, 1.45069297e-12, 1.25860544e-13, 4.27300475e-13], dtype=float32) . | db(db_dim_0)float32-1.054array([-1.0535065], dtype=float32) . | probs_species(probs_species_dim_0)float320.0 0.0 0.0 0.0 ... 1.0 1.0 1.0 1.0array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32) . | . | Attributes: (4)created_at :2022-06-14T12:24:15.418491arviz_version :0.12.1inference_library :pyroinference_library_version :1.8.0+0ec1e87 | . . | .",
            "url": "https://gstechschulte.github.io/cached-projects/jupyter/2022/06/14/bmcp-ch-3.html",
            "relUrl": "/jupyter/2022/06/14/bmcp-ch-3.html",
            "date": " • Jun 14, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Probabilistic Prediction Problems - Part 1",
            "content": "Part one deals with why parameters (or latent variables) are problematic and common loss functions for comparing two probability distributions. . The Problem with Parameters . Three main monsters when it comes to “modeling”: . Overfitting | Underfitting | Con-founders | The goal of the model should be stated before you choose your methods to tame these monsters: . Is the goal predictive power? | Is the goal to understand causes? | . Regarding monster (1), adding variables and parameters to a model can help to reveal hidden effects and improve estimates. However, more parameters always results in a better model “fit”. While more complex models fit the data better, they often predict new data worse. Models that have many parameters tend to overfit more than simpler models. Generally, fit is measured by how well the model can retrodict the data used to fit the model. A common metric for this is “variance explained”, $R^2$. Monster (2) hurts, too. Underfitting produces models that are inaccurate both within and out of sample. Underfit models have learned too little, whether that be from uninformative features or too simple a model. . So, how to navigate overfitting and underfitting? First, pick a criterion of model performance as the target—what do you want the model to be good at? Methods based on information theory can provide a common and useful target. . Distributions over Actions . It is possible to assume the set of possible actions is to pick a class label (or “reject” option) in classification or a real valued scalar as in a point estimate of a parameter. However, it is also possible to assume the set of possible actions is to pick a probability distribution over some value of interest (paramter or class label). That is, we want to perform probabilistic prediction or probabilistic forecasting rather than predicting a single value / scalar. From a decision theoretic approach, we assume the true state of nature is a distribution, $h = p(Y vert x)$, with the action being another distribution, $a = q(Y vert x)$. The goal is to be as close to the true state of nature $p$ with our approximation $q$. Therefore, we want to pick $q$ to minimize $ mathbb{E} ell(p, q)$ for a given $x$. . KL-Divergence, Cross-Entropy, and Log-Loss . KL-divergence, cross-entropy, and log-loss are common loss functions are comparing two distributions. . Scores and Rules . Proper Scoring Rules . Maximizing a proper scoring rule will force the model to match the true probabilities, i.e., the probabilities are calibrated. For example, when the weather person states the probability of rain is $70 %$, then it should rain about $70 %$ of the time. . The following are proper scoring “loss functions”: . Negative log-likelihood | Cross entropy | Brier score | . Evaluating Predictive Models . Log-scoring rule | . Model Selection and Evaluation . To check the results of modeling and inference, we would like to know how well a model fits observed data $x$, which we can quantify with the evidence or marginal likelihood. .",
            "url": "https://gstechschulte.github.io/cached-projects/probability/2022/05/31/Probabilistic-Predicion-Problems-Part-1.html",
            "relUrl": "/probability/2022/05/31/Probabilistic-Predicion-Problems-Part-1.html",
            "date": " • May 31, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Textbooks Have Gotten Good, Like Really Good",
            "content": "In a recent interview, they presented the problem they were facing and asked about the methods I would use to go about solving it. I started off with the heuristics behind the method (Bayesian inference if you are curious) and continued into some of the more detailed advantages and disadvantages behind why I would choose this method. I think they were a bit surprised about this as my original background is in economics, which is traditionally a domain that employs frequentist based methodologies. This led the interviewers to asking, “It seems you are quite familiar with Bayesian statistics and given your background, how did you come to using Bayesian methods within your projects?”. To which I responded, “I first read about the concept in Thinking Fast and Slow by Daniel Kahneman and became deeply intrigued, so I read a few textbooks about Bayesian statistics and Bayesian methods in machine learning.” The interviewers responses, “You read textbooks?”. . Initially, I was a little confused as their response inclined that reading technical textbooks is something that is not normal and or only the advanced have the capabilities to do. So I started to ask myself “How am I able to learn from such technical textbooks?” and started to swipe through the pdf textbooks from various authors and publishing dates. I came to the realization that the textbooks I most often read or refer back to are the ones that were recently published. This observation lead to the conclusion that modern textbooks are, well, very modern and interactive—directional linking throughout sections, hyperlinks to GitHub repositories and Google Colab notebooks for reproducible code, and excellent visualizations of the concepts presented throughout that chapter. . The improvement in technical textbooks is really a result of three ingredients: . 1.) Utilizing Jupyter Notebooks / Google Colab | 2.) Incorporating GitHub public repositories | 3.) The author(s) and or community providing code for the chapter’s content | . Kevin Murphy’s latest release “Probabilistic Machine Learning - An Introduction” has it’s own GitHub repository with Google Colab notebooks for generating figures for each chapter presented in the book. Not only that, but also supplementary material links for each chapter with additional information, typically Google Colab notebooks, showcasing the software libraries used to generate the figures. . When these three ingredients are brought together, you now have an interactive textbook in which some of the barriers to entry, in the form of prior knowledge and skillsets, are broken down and allow for a wider audience to gain said new knowledge and skillsets. Furthermore, and where traditional non-interactive textbooks lack, an interactive textbook’s main advantages are in the form of trial and error, math formalisms $ rightarrow$ code, and indirect efficient programming principles. . The mysterious power of trial and error could be the biggest advantage. Simply playing around with the code, inputing different values here and there to see how the outputs are affected allows one to gain a level of intuition that is not as easily accessible with textbooks without code or visualizations. Try something. It might not work and if it doesn’t, try something else—an error is just another opportunity to run another trial. . After having read the definitions and formalisms, going from math $ rightarrow$ code can be a difficult task and often renders the question, “where do I even start?”. . Building off of math $ rightarrow$ code, interactive textbooks allow one to see how others have programmed the algorithm. For me, as a self taught programmer, this is a huge value add as it allows me to learn more efficient and better programming principles (assuming the open source code is “efficient”). As well, reading code produced by others allows one to continually develop their skillsets and knowledge within programming—another valuable effect. . These three ingredients is where the bulk of the value add comes. A couple other points worth pointing out are (1) familiarizing yourself with the formalisms pays dividends, and (2) you should read the textbooks where the author resonates with you. Becoming knowledgeable with the formalisms pays dividends as you read other textbooks, research papers, and even venture outside of the field you are in. Their may be some subtleties in notations, but you can recognize this and still manage your way through the technical content you are reading. Secondly, it is often the case I will have three to four books about the same “material”, but I will only “connect” with one or two of the author’s way of describing the concepts. So, don’t be afraid to download several books on the same material (which is easily doable from pdf drive). .",
            "url": "https://gstechschulte.github.io/cached-projects/2021/09/14/Textbooks-Have-Gotten-Good,-Like-Really-Good.html",
            "relUrl": "/2021/09/14/Textbooks-Have-Gotten-Good,-Like-Really-Good.html",
            "date": " • Sep 14, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "No Code, Dependency, and Building Technology",
            "content": "Modernity and Abstraction . ‘Programmers’, loosely speaking, in some form or another have always been developing software to automate tedious and repetitive tasks. Rightly so, as this is one of the tasks computers are designed to perform. As science and technology progresses, and gets more technological, there is a growing seperation between the maker and the user. This is one of the negative externalities of modernism - we enjoy the benefits of a more advanced and technologically adept society, but fewer and fewer people understand the inner workings. Andrej Karpathy has a jokingly short paragraph in his blog on the matter, “A courageous developer has taken the burden of understanding query strings, urls, GET/POST requests, HTTP connections, and so on from you and largely hidden the complexity behind a few lines of code. This is what we are now familiar with and expect”. . Ever since the rise of machine learning and data science, the industry was bound to develop no and or low code products for everything between data cleaning, processing, and annotation, to the implementation of models. This is an example of one of the highest forms of abstraction - you don’t even need to be able to code. Knowledge of the problem at hand and some simple intuition into which model may provide a low validation error is almost all you need. A lower level form of abstraction is through the use of application programming interfaces (APIs) such as scikit-learn, PyTorch, etc. Using these APIs requires more technical skillsets, knowledge of first principles, and a deeper understanding of the problem than the no-code substitutes stated above. Lastly, we have the ‘bare bones’ implementation of algorithms - what most people usually call, “programming things from scratch”. But even then, good luck writing your favorite model without using numpy, scipy, or JAX. . As teams of couragous developers and organizations seek to create products to help make everyday routines a commodity and more productive, it can be harder to learn technical methods from first principles as they have been abstracted away. There’s no need, nor the time, to start and go writing everything from scratch, but having a solid intuition into what is going on under the hood can allow you to uniquely solve complex problems, debug more efficiently, contribute to open source software, etc. . Switching Costs, Dependency and Open Source Goods . At least the two lower levels of abstractions usually rely on open-source software, whereas the no-code alternative is typically (at the moment) proprietary technology in the form of machine learning as a service and or platform. [Edit 14.09.2021: H20.ai, HuggingFace, MakeML, CreateML, Google Cloud AutoML are all open-source services or platforms in the low or no code ML space] Open-source gives you the biggest flexibility that, if the space again changes, you can move things. Otherwise, you find yourself locked into a technology stack the way you were locked in to technologies from the ’80s and ’90s and 2000s. . Being locked into IT components can have serious opportunity costs. For example, switching from Mac to a Windows based PC involves not only the hardware costs of the computer itself, but also involves purchasing of a whole new library of software, and even more importantly, learning how to use a brand new system. When you, or an organization, decides to go the propriety route, these switching costs can be very high, and users may find themselves experiencing lock-in; a situation where the cost of changing to a different system is so high that switching is virtually inconcievable. . Of course, the producers of this hardware / sofware love the fact that you have an inelastic demand curve - a rise in prices won’t affect demand much as switching costs are high. In summary, as machine learning platforms didn’t really exist ~15 years ago, they will more than likely change quite a bit and you are dependent on the producer of continually adapting to industry trends and technological advancements while at the same time, staying flexible to cater to your edge cases when you need it. . Leverage and Building Technology . It would be a full time job to stay present and up to date on everything being released in the space of “machine learning”, but also to be knowledgeable of first princples and have skillsets to use the technologies is in another class of its own. Before AWS, APIs, open source, etc., as an organization or startup, it was likely the question, “we need amazing technical people who can build it all”. Now, with the increase and rise of PaaS, SaaS, open source librarys and tooling, the question shifts from a question of “building”, to a question of “leveraging existing tools”. How long before no / low code gets good enough before we are asking, “why did we not build this with no-code tools?”. . The highest form of leverage for a company is to develop and build out difficult and new technology. No-code, if developed right (and is ideally open-source), can still provide leverage and value-add, but any advantage just becomes table stakes. This is what will distinguish great teams vs. good teams; non-linear returns will continue to be through building out proprietery and difficult technology. .",
            "url": "https://gstechschulte.github.io/cached-projects/2021/08/10/No-Code-Dependency-and-Building-Technology.html",
            "relUrl": "/2021/08/10/No-Code-Dependency-and-Building-Technology.html",
            "date": " • Aug 10, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Economics of Open Source",
            "content": "Why does open-source work? . Highly skilled people devoting their valuable time for little to no monetary benefit. . . . . .",
            "url": "https://gstechschulte.github.io/cached-projects/2021/05/31/Economics-of-Open-Source.html",
            "relUrl": "/2021/05/31/Economics-of-Open-Source.html",
            "date": " • May 31, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Intelligent Systems",
            "content": "Infrastructure, Transportation, . | Multiple decisions at scale. . | Uber, AirBnB, Music . | Producer - consumer relationships $ rightarrow$ no advertising . | .",
            "url": "https://gstechschulte.github.io/cached-projects/2021/05/31/Data-Based-Marketplaces.html",
            "relUrl": "/2021/05/31/Data-Based-Marketplaces.html",
            "date": " • May 31, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://gstechschulte.github.io/cached-projects/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is a home for frequently thought about projects, ideas, and or methodologies related to technology, probabilistic machine learning, and microeconomics. This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://gstechschulte.github.io/cached-projects/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gstechschulte.github.io/cached-projects/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}