{
  
    
        "post0": {
            "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
            "content": "from utilities import utils import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np from scipy import stats import arviz as az import torch import pyro import pyro.distributions as dist from pyro.distributions import constraints from pyro.infer import Predictive, TracePredictive, NUTS, MCMC from pyro.infer.autoguide import AutoLaplaceApproximation from pyro.infer import SVI, Trace_ELBO from pyro.optim import Adam from pyro.infer.mcmc.util import summary import os plt.style.use(&#39;ggplot&#39;) . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html from .autonotebook import tqdm as notebook_tqdm . babies = pd.read_csv(os.path.abspath(&#39;.&#39;) + &#39;/data/babies.csv&#39;) babies . Month Length . 0 0 | 48.5 | . 1 0 | 50.5 | . 2 0 | 50.5 | . 3 0 | 52.0 | . 4 0 | 47.5 | . ... ... | ... | . 795 24 | 87.5 | . 796 24 | 82.5 | . 797 24 | 88.5 | . 798 24 | 89.0 | . 799 24 | 87.0 | . 800 rows × 2 columns . 4.1 Transforming Covariates . Figure 4.2 . X_ = torch.from_numpy(babies[&#39;Month&#39;].values.reshape(-1, 1)) X_ = torch.tensor(X_).float() y = torch.from_numpy(babies[&#39;Length&#39;].values) y = torch.tensor(y).float() . /var/folders/5n/lzw120x534d6n5nbhk8qwzjr0000gs/T/ipykernel_37390/2039087259.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). X_ = torch.tensor(X_).float() /var/folders/5n/lzw120x534d6n5nbhk8qwzjr0000gs/T/ipykernel_37390/2039087259.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). y = torch.tensor(y).float() . def linear_babies(month, length=None): N, P = month.shape beta_0 = pyro.sample(&#39;beta_0&#39;, dist.Normal(0., 10.)) beta_1 = pyro.sample(&#39;beta_1&#39;, dist.Normal(0, 10.).expand([P])) sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(10.)) mu = beta_0 + torch.matmul(beta_1, month.T) with pyro.plate(&#39;plate&#39;, size=N): y = pyro.sample(&#39;y&#39;, dist.Normal(mu, sigma), obs=length) . pyro.render_model( linear_babies, model_args=(X_, y), render_distributions=True ) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_plate plate beta_0 beta_0 y y beta_0&#45;&gt;y beta_1 beta_1 beta_1&#45;&gt;y sigma sigma sigma&#45;&gt;y distribution_description_node beta_0 ~ Normal beta_1 ~ Normal sigma ~ HalfNormal y ~ Normal kernel = NUTS(linear_babies, adapt_step_size=True) mcmc_linear_babies = MCMC(kernel, 500, 300) mcmc_linear_babies.run(X_, y) . Sample: 100%|██████████| 800/800 [00:11, 71.18it/s, step size=4.08e-01, acc. prob=0.921] . mcmc_babie_samples = mcmc_linear_babies.get_samples(1000) predictive = Predictive(linear_babies, mcmc_babie_samples)(X_, None) az_linear_babies = az.from_pyro( posterior=mcmc_linear_babies, posterior_predictive=predictive) . posterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented. . y_mu = predictive[&#39;y&#39;].mean(axis=0) . plt.figure(figsize=(10, 6)) sns.scatterplot(x=babies[&#39;Month&#39;], y=babies[&#39;Length&#39;], color=&#39;grey&#39;, alpha=0.75) sns.lineplot(x=babies[&#39;Month&#39;], y=y_mu, color=&#39;blue&#39;) az.plot_hdi(x=babies[&#39;Month&#39;], y=predictive[&#39;y&#39;].numpy()) plt.show() . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs) . Figure 4.3 . def sqrt_babies(month, length=None): N, P = month.shape beta_0 = pyro.sample(&#39;beta_0&#39;, dist.Normal(0., 10.)) beta_1 = pyro.sample(&#39;beta_1&#39;, dist.Normal(0, 10.).expand([P])) sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(10.)) mu = beta_0 + torch.matmul(beta_1, torch.sqrt(month.T)) with pyro.plate(&#39;plate&#39;, size=N): y = pyro.sample(&#39;y&#39;, dist.Normal(mu, sigma), obs=length) . kernel = NUTS(sqrt_babies, adapt_step_size=True) mcmc_sqrt = MCMC(kernel, 500, 300) mcmc_sqrt.run(X_, y) . Sample: 100%|██████████| 800/800 [00:13, 58.54it/s, step size=3.43e-01, acc. prob=0.926] . mcmc_sqrt_babie_samples = mcmc_sqrt.get_samples(1000) predictive_sqrt = Predictive(sqrt_babies, mcmc_sqrt_babie_samples)(X_, None) az_sqrt_babies = az.from_pyro( posterior=mcmc_sqrt, posterior_predictive=predictive_sqrt) . posterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented. . y_mu = predictive_sqrt[&#39;y&#39;].mean(axis=0) . plt.figure(figsize=(10, 6)) plt.scatter(babies[&#39;Month&#39;], babies[&#39;Length&#39;], c=&#39;black&#39;, alpha=0.5) plt.plot(babies[&#39;Month&#39;], y_mu, color=&#39;blue&#39;) az.plot_hdi(x=babies[&#39;Month&#39;], y=az_sqrt_babies[&#39;posterior_predictive&#39;][&#39;y&#39;], hdi_prob=.50, color=&#39;grey&#39;) az.plot_hdi(x=babies[&#39;Month&#39;], y=az_sqrt_babies[&#39;posterior_predictive&#39;][&#39;y&#39;], hdi_prob=.94, color=&#39;darkgrey&#39;) plt.title(&#39;Linear model with square root transformation on months&#39;) plt.show() . 4.2 - Varying Uncertainty . 4.3 - Interaction Effects . tips_df = pd.read_csv(os.path.abspath(&#39;.&#39;) + &#39;/data/tips.csv&#39;) tips_df . total_bill tip sex smoker day time size . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | . ... ... | ... | ... | ... | ... | ... | ... | . 239 29.03 | 5.92 | Male | No | Sat | Dinner | 3 | . 240 27.18 | 2.00 | Female | Yes | Sat | Dinner | 2 | . 241 22.67 | 2.00 | Male | Yes | Sat | Dinner | 2 | . 242 17.82 | 1.75 | Male | No | Sat | Dinner | 2 | . 243 18.78 | 3.00 | Female | No | Thur | Dinner | 2 | . 244 rows × 7 columns . total_bill_centered = torch.tensor((tips_df[&quot;total_bill&quot;] - tips_df[&quot;total_bill&quot;].mean()).values, dtype=torch.float64) tips = torch.tensor(tips_df[&quot;tip&quot;].values, dtype=torch.float64) smoker = torch.tensor(pd.Categorical(tips_df[&quot;smoker&quot;]).codes, dtype=torch.float64) . def interaction_model(bill, smoker, tips=None): #N, P = beta = pyro.sample(&#39;beta&#39;, dist.Normal(0., 1.).expand([4])) sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(1.)) mu = beta[0] + beta[1] * bill + beta[2] * smoker + beta[3] * smoker * bill with pyro.plate(&#39;plate&#39;, len(bill)): y = pyro.sample(&#39;y&#39;, dist.Normal(mu, sigma), obs=tips) . pyro.render_model( interaction_model, (total_bill_centered, smoker, tips), render_distributions=True) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_plate plate beta beta y y beta&#45;&gt;y sigma sigma sigma&#45;&gt;y distribution_description_node beta ~ Normal sigma ~ HalfNormal y ~ Normal kernel = NUTS(interaction_model, adapt_step_size=True) mcmc_interaction = MCMC(kernel, 500, 300) mcmc_interaction.run(total_bill_centered, smoker, tips) . Sample: 100%|██████████| 800/800 [00:11, 72.34it/s, step size=5.10e-01, acc. prob=0.889] . mcmc_interaction_samples = mcmc_interaction.get_samples(1000) interaction_predictive = Predictive(interaction_model, mcmc_interaction_samples) posterior_predictive = interaction_predictive(total_bill_centered, smoker, None) az_inference_interaction = az.from_pyro( posterior=mcmc_interaction, posterior_predictive=posterior_predictive) . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/data/io_pyro.py:157: UserWarning: Could not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False warnings.warn( posterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented. . tip_mu = posterior_predictive[&#39;y&#39;].mean(axis=0) tip_std = posterior_predictive[&#39;y&#39;].std(axis=0) predictions = pd.DataFrame({ &#39;bill&#39;: total_bill_centered, &#39;smoker&#39;: smoker, &#39;tip&#39;: tips, &#39;tip_mu&#39;: tip_mu, &#39;tip_std&#39;: tip_std, &#39;tip_high&#39;: tip_mu + tip_std, &#39;tip_low&#39;: tip_mu - tip_std }) predictions = predictions.sort_values(by=[&#39;bill&#39;]) . smoker_df = predictions[predictions[&#39;smoker&#39;] == 1] nonsmoker_df = predictions[predictions[&#39;smoker&#39;] == 0] . plt.figure(figsize=(10, 6)) sns.lineplot(x=smoker_df[&#39;bill&#39;], y=smoker_df[&#39;tip_mu&#39;], color=&#39;blue&#39;) plt.fill_between(smoker_df[&#39;bill&#39;], smoker_df[&#39;tip_low&#39;], smoker_df[&#39;tip_high&#39;], color=&#39;lightblue&#39;) sns.lineplot(x=nonsmoker_df[&#39;bill&#39;], y=nonsmoker_df[&#39;tip_mu&#39;], color=&#39;red&#39;) plt.fill_between(nonsmoker_df[&#39;bill&#39;], nonsmoker_df[&#39;tip_low&#39;], nonsmoker_df[&#39;tip_high&#39;], color=&#39;lightgrey&#39;) sns.scatterplot(x=predictions[&#39;bill&#39;], y=predictions[&#39;tip&#39;], hue=predictions[&#39;smoker&#39;]) plt.legend([&#39;smoker&#39;, &#39;non-smoker&#39;]) plt.title(&#39;Interaction Effect on Tip&#39;) plt.show() . 4.4 - Robust Regression . def generate_sales(*, days, mean, std, label): &quot;&quot;&quot;code taken from the authors / book&quot;&quot;&quot; np.random.seed(0) df = pd.DataFrame(index=range(1, days+1), columns=[&quot;customers&quot;, &quot;sales&quot;]) for day in range(1, days+1): num_customers = stats.randint(30, 100).rvs()+1 # This is correct as there is an independent draw for each customers orders dollar_sales = stats.norm(mean, std).rvs(num_customers).sum() df.loc[day, &quot;customers&quot;] = num_customers df.loc[day, &quot;sales&quot;] = dollar_sales # Fix the types as not to cause Theano errors df = df.astype({&#39;customers&#39;: &#39;int32&#39;, &#39;sales&#39;: &#39;float32&#39;}) # Sorting will make plotting the posterior predictive easier later df[&quot;Food_Category&quot;] = label df = df.sort_values(&quot;customers&quot;) return df . empanadas = generate_sales(days=200, mean=180, std=30, label=&quot;Empanada&quot;) empanadas.iloc[0] = [50, 92000, &quot;Empanada&quot;] empanadas.iloc[1] = [60, 90000, &quot;Empanada&quot;] empanadas.iloc[2] = [70, 96000, &quot;Empanada&quot;] empanadas.iloc[3] = [80, 91000, &quot;Empanada&quot;] empanadas.iloc[4] = [90, 99000, &quot;Empanada&quot;] empanadas = empanadas.sort_values(&quot;customers&quot;) fig, ax = plt.subplots(figsize=(10, 6)) empanadas.sort_values(&quot;sales&quot;)[:-5].plot(x=&quot;customers&quot;, y=&quot;sales&quot;, kind=&quot;scatter&quot;, ax=ax); empanadas.sort_values(&quot;sales&quot;)[-5:].plot(x=&quot;customers&quot;, y=&quot;sales&quot;, kind=&quot;scatter&quot;, c=&quot;black&quot;, ax=ax); ax.set_ylabel(&quot;Argentine Peso&quot;) ax.set_xlabel(&quot;Customer Count&quot;) ax.set_title(&quot;Empanada Sales&quot;) . Text(0.5, 1.0, &#39;Empanada Sales&#39;) . customer_count = torch.tensor(empanadas[&#39;customers&#39;].values, dtype=torch.float64) sales = torch.tensor(empanadas[&#39;sales&#39;].values, dtype=torch.float64) . def robust_regression(customers, peso=None): sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(50.)) beta = pyro.sample(&#39;beta&#39;, dist.Normal(150., 20.)) v = pyro.sample(&#39;dof&#39;, dist.HalfNormal(20.)) mu = pyro.deterministic(&#39;mu&#39;, beta * customers) with pyro.plate(&#39;plate&#39;, len(customers)): sales = pyro.sample(&#39;sales&#39;, dist.StudentT(loc=mu, scale=sigma, df=v), obs=peso) . pyro.render_model( robust_regression, (customer_count, sales), render_distributions=True) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_plate plate sigma sigma sales sales sigma&#45;&gt;sales beta beta beta&#45;&gt;sales dof dof dof&#45;&gt;sales mu mu distribution_description_node sigma ~ HalfNormal beta ~ Normal dof ~ HalfNormal mu ~ Delta sales ~ StudentT kernel = NUTS(robust_regression) mcmc_robust = MCMC(kernel, 500, 300) mcmc_robust.run(customer_count, sales) . Sample: 100%|██████████| 800/800 [00:15, 52.03it/s, step size=7.37e-01, acc. prob=0.925] . mcmc_robust.summary() . mean std median 5.0% 95.0% n_eff r_hat beta 179.59 0.26 179.60 179.11 179.97 348.75 1.00 dof 1.29 0.16 1.27 1.02 1.54 406.80 1.00 sigma 152.30 12.89 152.18 129.69 171.41 431.98 1.00 Number of divergences: 0 . mcmc_robust_samples = mcmc_robust.get_samples(1000) robust_predictive = Predictive(robust_regression, mcmc_robust_samples)(customer_count, None) az_robust_inf = az.from_pyro(posterior=mcmc_robust, posterior_predictive=robust_predictive) . posterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented. posterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented. . mu = az_robust_inf[&#39;posterior_predictive&#39;][&#39;mu&#39;].values.reshape(-1, len(customer_count)).mean(axis=0) plt.figure(figsize=(10, 6)) plt.scatter(customer_count, sales) plt.plot(customer_count, mu, c=&#39;blue&#39;) az.plot_hdi(customer_count, az_robust_inf[&#39;posterior_predictive&#39;][&#39;sales&#39;], color=&#39;grey&#39;) plt.xlabel(&#39;Customer Count&#39;) plt.ylabel(&#39;Sales&#39;) plt.title(&#39;Robust Regression using Student-t likelihood&#39;) plt.show() . 4.5 - Pooling, Multilevel Models, Mixed Effects . def generate_sales(*, days, mean, std, label): &quot;&quot;&quot;code taken from authors / book&quot;&quot;&quot; np.random.seed(0) df = pd.DataFrame(index=range(1, days+1), columns=[&quot;customers&quot;, &quot;sales&quot;]) for day in range(1, days+1): num_customers = stats.randint(30, 100).rvs()+1 # This is correct as there is an independent draw for each customers orders dollar_sales = stats.norm(mean, std).rvs(num_customers).sum() df.loc[day, &quot;customers&quot;] = num_customers df.loc[day, &quot;sales&quot;] = dollar_sales # Fix the types as not to cause Theano errors df = df.astype({&#39;customers&#39;: &#39;int32&#39;, &#39;sales&#39;: &#39;float32&#39;}) # Sorting will make plotting the posterior predictive easier later df[&quot;Food_Category&quot;] = label df = df.sort_values(&quot;customers&quot;) return df . pizza_df = generate_sales(days=365, mean=13, std=5, label=&quot;Pizza&quot;) sandwich_df = generate_sales(days=100, mean=6, std=5, label=&quot;Sandwich&quot;) salad_days = 3 salad_df = generate_sales(days=salad_days, mean=8 ,std=3, label=&quot;Salad&quot;) . fig, ax = plt.subplots(figsize=(10, 6)) pizza_df.plot(x=&quot;customers&quot;, y=&quot;sales&quot;, kind=&quot;scatter&quot;, ax=ax, c=&quot;grey&quot;, label=&quot;Pizza&quot;, marker=&quot;^&quot;, s=60); sandwich_df.plot(x=&quot;customers&quot;, y=&quot;sales&quot;, kind=&quot;scatter&quot;, ax=ax, c=&#39;black&#39;, label=&quot;Sandwich&quot;, marker=&quot;s&quot;); salad_df.plot(x=&quot;customers&quot;, y=&quot;sales&quot;, kind=&quot;scatter&quot;, ax=ax, label=&quot;Salad&quot;, c=&quot;blue&quot;); ax.set_xlabel(&quot;Number of Customers&quot;) ax.set_ylabel(&quot;Daily Sales Dollars&quot;) ax.set_title(&quot;Aggregated Sales Dollars&quot;) ax.legend() plt.show() . sales_df = pd.concat([pizza_df, sandwich_df, salad_df]).reset_index(drop=True) sales_df[&quot;Food_Category&quot;] = pd.Categorical(sales_df[&quot;Food_Category&quot;]) sales_df . customers sales Food_Category . 0 31 | 459.895203 | Pizza | . 1 31 | 401.147736 | Pizza | . 2 31 | 413.345245 | Pizza | . 3 31 | 371.909241 | Pizza | . 4 32 | 433.797089 | Pizza | . ... ... | ... | ... | . 463 100 | 611.736816 | Sandwich | . 464 100 | 667.152954 | Sandwich | . 465 42 | 331.625702 | Salad | . 466 66 | 520.900940 | Salad | . 467 75 | 628.937622 | Salad | . 468 rows × 3 columns . customers = torch.tensor(sales_df[&#39;customers&#39;].values, dtype=torch.float64) sales = torch.tensor(sales_df[&#39;sales&#39;].values, dtype=torch.float64) food_category = torch.tensor(sales_df[&#39;Food_Category&#39;].cat.codes.values, dtype=torch.long) . Notes: . extend shape to 3 because of the 3 food categories | use dtype = torch.long when using a tensor as indices | if you use the pyro.plate() primitive, it seems you do not need to specify the .expand() method on distributions, i.e., to make the batch size &gt; 1 in the case of a multidimensional design matrix | . Unpooled - MCMC . def unpooled_model(food_cat, customers, sales=None): P = 3 N = len(customers) with pyro.plate(&#39;food_cat_i&#39;, len(np.unique(food_cat))): sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(20.)) beta = pyro.sample(&#39;beta&#39;, dist.Normal(10., 10.)) with pyro.plate(&#39;data&#39;, N): mu = pyro.deterministic(&#39;mu&#39;, beta[food_cat] * customers) output = pyro.sample(&#39;y&#39;, dist.Normal(mu, sigma[food_cat]), obs=sales) . pyro.render_model( unpooled_model, (food_category, customers, sales), render_distributions=True ) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_food_cat_i food_cat_i cluster_data data sigma sigma y y sigma&#45;&gt;y beta beta beta&#45;&gt;y mu mu distribution_description_node sigma ~ HalfNormal beta ~ Normal mu ~ Delta y ~ Normal kernel = NUTS(unpooled_model) mcmc_unpooled = MCMC(kernel, 500, 300) mcmc_unpooled.run(food_category, customers, sales) . Sample: 100%|██████████| 800/800 [00:10, 77.79it/s, step size=6.71e-01, acc. prob=0.880] . mcmc_unpooled.summary() . mean std median 5.0% 95.0% n_eff r_hat beta[0] 13.02 0.03 13.02 12.97 13.07 602.33 1.00 beta[1] 8.15 0.21 8.14 7.79 8.46 173.19 1.00 beta[2] 6.11 0.05 6.11 6.03 6.20 622.87 1.00 sigma[0] 40.11 1.39 39.99 37.96 42.48 551.20 1.00 sigma[1] 21.88 10.01 19.39 7.71 34.39 215.86 1.00 sigma[2] 35.61 2.48 35.55 31.86 39.67 491.78 1.00 Number of divergences: 0 . unpooled_posterior_samples = mcmc_unpooled.get_samples(1000) unpooled_predictive = Predictive(unpooled_model, unpooled_posterior_samples)(food_category, customers, None) az_unpooled_inf = az.from_pyro( posterior=mcmc_unpooled, posterior_predictive=unpooled_predictive) . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/data/io_pyro.py:157: UserWarning: Could not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False warnings.warn( posterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented. posterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented. . az.plot_trace(az_unpooled_inf, var_names=[&quot;beta&quot;, &quot;sigma&quot;], compact=False) plt.tight_layout() . az.plot_forest(az_unpooled_inf, var_names=[&#39;beta&#39;]) plt.show() . az.plot_forest(az_unpooled_inf, var_names=[&#39;sigma&#39;]) plt.show() . Unpooled - SVI . NOT FINISHED . def unpooled_model(food_cat, customers, sales=None): P = 3 N = len(customers) with pyro.plate(&#39;food_cat_i&#39;, len(np.unique(food_cat))): sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(20.)) beta = pyro.sample(&#39;beta&#39;, dist.Normal(10., 10.)) with pyro.plate(&#39;data&#39;, N): mu = pyro.deterministic(&#39;mu&#39;, beta[food_cat] * customers) output = pyro.sample(&#39;y&#39;, dist.Normal(mu, sigma[food_cat]), obs=sales) . def unpooled_guide(food_cat, customers, sales=None): with pyro.plate(&#39;food_cat_i&#39;, len(np.unique(food_cat))): sigma_scale = pyro.param( &#39;sigma_scale&#39;, torch.tensor(1.), constraint=constraints.positive ) sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(sigma_scale)) beta_loc = pyro.param(&#39;beta_loc&#39;, torch.tensor(10.)) beta_scale = pyro.param( &#39;beta_scale&#39;, torch.tensor(1.), constraint=constraints.positive) beta = pyro.sample(&#39;beta&#39;, dist.Normal(beta_loc, beta_scale)) . pyro.render_model( unpooled_guide, (food_category, customers, sales), render_params=True) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_food_cat_i food_cat_i beta_loc beta_loc beta beta beta_loc&#45;&gt;beta sigma_scale sigma_scale sigma sigma sigma_scale&#45;&gt;sigma beta_scale beta_scale beta_scale&#45;&gt;beta pyro.clear_param_store() #adam_params = {&#39;lr&#39;: 0.005, &#39;betas&#39;: (0.95, 0.99)} adam_params = {&#39;lr&#39;: 0.005} optim = Adam(adam_params) svi = SVI(unpooled_model, unpooled_guide, optim, Trace_ELBO()) iter = 1000 elbo_loss = [] for i in range(iter): loss = svi.step(food_category, customers_z, sales_z) elbo_loss.append(loss) plt.figure(figsize=(10, 3)) plt.plot(np.arange(1, iter+1), elbo_loss) plt.ylabel(&#39;ELBO Loss&#39;) plt.xlabel(&#39;Iterations&#39;) plt.title(f&#39;iter {len(elbo_loss)}, loss: {elbo_loss[-1]:.4f}&#39;) plt.show() . for name, value in pyro.get_param_store().items(): print(name, pyro.param(name).data.cpu().numpy()) . predictive = Predictive(unpooled_model, guide=unpooled_guide, num_samples=1000) posterior_svi_samples = predictive(food_category, customers, None) . sigma, beta = utils.summary(posterior_svi_samples)[&#39;sigma&#39;], utils.summary(posterior_svi_samples)[&#39;beta&#39;] . Pooled - MCMC . def pooled_model(food_cat, customers, sales=None): P = 3 N = len(customers) sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(20.)) beta = pyro.sample(&#39;beta&#39;, dist.Normal(10., 10.)) with pyro.plate(&#39;data&#39;, N): mu = pyro.deterministic(&#39;mu&#39;, beta * customers) output = pyro.sample(&#39;y&#39;, dist.Normal(mu, sigma), obs=sales) . kernel = NUTS(pooled_model) mcmc_pooled = MCMC(kernel, 500, 300) mcmc_pooled.run(food_category, customers, sales) . Sample: 100%|██████████| 800/800 [00:05, 148.29it/s, step size=9.55e-01, acc. prob=0.897] . mcmc_pooled.summary() . mean std median 5.0% 95.0% n_eff r_hat beta 11.50 0.12 11.50 11.30 11.70 260.53 1.00 sigma 186.27 4.99 186.11 178.25 194.44 487.80 1.00 Number of divergences: 0 . pooled_samples = mcmc_pooled.get_samples(1000) pooled_predictive = Predictive(pooled_model, pooled_samples)(food_category, customers, None) az_pooled_inf = az.from_pyro( posterior=mcmc_pooled, posterior_predictive=pooled_predictive) . posterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented. posterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented. . sales_mu = pooled_predictive[&#39;y&#39;].mean(axis=0) sales_std = pooled_predictive[&#39;y&#39;].std(axis=0) predictions = pd.DataFrame({ &#39;customers&#39;: customers, &#39;category&#39;: food_category, &#39;sales&#39;: sales, &#39;sales_mu&#39;: sales_mu, &#39;sales_std&#39;: sales_std, &#39;sales_high&#39;: sales_mu + sales_std, &#39;sales_low&#39;: sales_mu - sales_std }) predictions = predictions.sort_values(by=[&#39;customers&#39;]) . plt.figure(figsize=(10, 6)) sns.scatterplot( x=predictions[&#39;customers&#39;], y=predictions[&#39;sales&#39;], hue=predictions[&#39;category&#39;], palette=&#39;tab10&#39;) sns.lineplot( x=predictions[&#39;customers&#39;], y=predictions[&#39;sales_mu&#39;], color=&#39;black&#39;) plt.fill_between( x=predictions[&#39;customers&#39;], y1=predictions[&#39;sales_low&#39;], y2=predictions[&#39;sales_high&#39;], color=&#39;grey&#39;, alpha=0.25) plt.title(&#39;Pooled Parameters - MCMC&#39;) plt.show() . Pooled - SVI . Using LaPlace Approximation doesn&#39;t make the most sense since. . . . pooled_guide learns beta, but does not learn sigma. This could be related to parameter initialization . def pooled_guide(customers, sales=None): sigma_scale = pyro.param( &#39;sigma_scale&#39;, 0.1 * torch.rand(1), constraint=constraints.positive ) sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(sigma_scale)) beta_loc = pyro.param(&#39;beta_loc&#39;, torch.tensor(1.)) beta_scale = pyro.param( &#39;beta_scale&#39;, 0.1 * torch.rand(1), constraint=constraints.positive) beta = pyro.sample(&#39;beta&#39;, dist.Normal(beta_loc, beta_scale)) . pyro.render_model( pooled_guide (customers, sales), render_params=True) . auto_guide = AutoLaplaceApproximation(pooled_model) pyro.render_model( auto_guide, (customers, sales), render_params=True) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_data data _AutoLaplaceApproximation_latent _AutoLaplaceApproximation_latent sigma sigma beta beta y y pyro.clear_param_store() #adam_params = {&#39;lr&#39;: 0.005, &#39;betas&#39;: (0.95, 0.99)} adam_params = {&#39;lr&#39;: 0.003} optim = Adam(adam_params) svi = SVI(pooled_model, auto_guide, optim, Trace_ELBO()) iter = 2000 elbo_loss = [] for i in range(iter): loss = svi.step(customers, sales) elbo_loss.append(loss) plt.figure(figsize=(10, 3)) plt.plot(np.arange(1, iter+1), elbo_loss) plt.ylabel(&#39;ELBO Loss&#39;) plt.xlabel(&#39;Iterations&#39;) plt.title(f&#39;iter {len(elbo_loss)}, loss: {elbo_loss[-1]:.4f}&#39;) plt.show() . predictive = Predictive(pooled_model, guide=auto_guide, num_samples=1000) posterior_svi_samples = predictive(customers, None) #sigma, beta = utils.summary(posterior_svi_samples)[&#39;sigma&#39;], # utils.summary(posterior_svi_samples)[&#39;beta&#39;] . sales_mu = posterior_svi_samples[&#39;y&#39;].mean(axis=0) sales_std = posterior_svi_samples[&#39;y&#39;].std(axis=0) predictions_svi = pd.DataFrame({ &#39;customers&#39;: customers, &#39;category&#39;: food_category, &#39;sales&#39;: sales, &#39;sales_mu&#39;: sales_mu, &#39;sales_std&#39;: sales_std, &#39;sales_high&#39;: sales_mu + sales_std, &#39;sales_low&#39;: sales_mu - sales_std }) predictions_svi = predictions_svi.sort_values(by=[&#39;customers&#39;]) . plt.figure(figsize=(10, 6)) sns.scatterplot( x=predictions_svi[&#39;customers&#39;], y=predictions_svi[&#39;sales&#39;], hue=predictions_svi[&#39;category&#39;], palette=&#39;tab10&#39;) sns.lineplot( x=predictions_svi[&#39;customers&#39;], y=predictions_svi[&#39;sales_mu&#39;], color=&#39;black&#39;) plt.fill_between( x=predictions_svi[&#39;customers&#39;], y1=predictions_svi[&#39;sales_low&#39;], y2=predictions_svi[&#39;sales_high&#39;], color=&#39;grey&#39;, alpha=0.25) plt.title(&#39;Pooled Parameters - SVI&#39;) plt.show() . Mixing Group and Common Parameters - MCMC . customers_z = (customers - customers.mean()) / (customers.std()) sales_std = sales / sales.max() . def pooled_sigma_model(food_cat, customers, sales=None): P = len(np.unique(food_cat)) N = len(customers) with pyro.plate(&#39;food_cat_i&#39;, P): beta = pyro.sample(&#39;beta&#39;, dist.Normal(10., 20.)) sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(20.)) with pyro.plate(&#39;data&#39;, N): mu = pyro.deterministic(&#39;mu&#39;, beta[food_cat] * customers) output = pyro.sample(&#39;y&#39;, dist.Normal(mu, sigma), obs=sales) . pyro.render_model( pooled_sigma_model, (food_category, customers, sales), render_distributions=True ) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_food_cat_i food_cat_i cluster_data data sigma sigma y y sigma&#45;&gt;y beta beta beta&#45;&gt;y mu mu distribution_description_node beta ~ Normal sigma ~ HalfNormal mu ~ Delta y ~ Normal kernel = NUTS(pooled_sigma_model) mcmc_pooled_sigma = MCMC(kernel, 500, 300) mcmc_pooled_sigma.run(food_category, customers, sales) . Sample: 100%|██████████| 800/800 [00:08, 90.60it/s, step size=7.63e-01, acc. prob=0.906] . mcmc_pooled_sigma.summary() . mean std median 5.0% 95.0% n_eff r_hat beta[0] 13.02 0.03 13.02 12.98 13.07 554.35 1.00 beta[1] 8.12 0.39 8.10 7.48 8.74 681.90 1.00 beta[2] 6.11 0.06 6.11 6.02 6.21 596.84 1.00 sigma 39.35 1.23 39.30 37.23 41.23 518.86 1.00 Number of divergences: 0 . pooled_sigma_samples = mcmc_pooled_sigma.get_samples(1000) pooled_sigma_predictive = Predictive(pooled_sigma_model, pooled_sigma_samples)(food_category, customers, None) az_pooled_sigma_inf = az.from_pyro( posterior=mcmc_pooled_sigma, posterior_predictive=pooled_sigma_predictive) . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/data/io_pyro.py:157: UserWarning: Could not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False warnings.warn( posterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented. posterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented. . sales_df[&#39;food_cat_encode&#39;] = food_category sales_df[&#39;sales_std&#39;] = sales_std sales_df[&#39;customers_z&#39;] = customers_z fig, ax = plt.subplots(figsize=(10, 6)) #az_pooled_sigma_inf[&#39;posterior&#39;][&#39;sigma&#39;].mean().values for i in range(3): category_mask = sales_df[&#39;food_cat_encode&#39;] == i mu_cat = pooled_sigma_predictive[&#39;y&#39;][:, category_mask].mean(axis=0) customers = sales_df.loc[category_mask, [&#39;customers&#39;]].values.flatten() sales = sales_df.loc[category_mask, [&#39;sales&#39;]].values.flatten() ax.plot(customers, mu_cat, c=&#39;black&#39;) ax.scatter(customers, sales) az.plot_hdi( x=customers, y=pooled_sigma_predictive[&#39;y&#39;][:, category_mask], color=&#39;grey&#39; ) ax.set_xlabel(&#39;Customers&#39;) ax.set_ylabel(&#39;Sales&#39;) ax.set_title(&#39;Pooled Sigma&#39;) . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs) /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs) /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs) . Text(0.5, 1.0, &#39;Pooled Sigma&#39;) . Mixing Group and Common Parameters - SVI . Using LaPlace approximation . customers_z = (customers - customers.mean()) / (customers.std()) sales_std = sales / sales.max() . def pooled_sigma_model(food_cat, customers, sales=None): P = len(np.unique(food_cat)) N = len(customers) with pyro.plate(&#39;food_cat_i&#39;, P): beta = pyro.sample(&#39;beta&#39;, dist.Normal(10., 20.)) sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(20.)) with pyro.plate(&#39;data&#39;, N): mu = pyro.deterministic(&#39;mu&#39;, beta[food_cat] * customers) output = pyro.sample(&#39;y&#39;, dist.Normal(mu, sigma), obs=sales) . from torch.distributions import constraints, transforms def pool_sigma_guide(food_cat, customers, sales=None): P = len(np.unique(food_cat)) N = len(customers) with pyro.plate(&#39;food_cat_i&#39;, P): beta_scale = pyro.param( &#39;beta_scale&#39;, torch.tensor(1.), constraint=constraints.positive) beta_loc = pyro.param(&#39;beta_loc&#39;, torch.randn(1)) beta = pyro.sample(&#39;beta&#39;, dist.Normal(beta_loc, beta_scale)) sigma_loc = pyro.param( &#39;sigma_loc&#39;, torch.randn(1)) sigma_scale = pyro.param( &#39;sigma_scale&#39;, 0.1 * torch.rand(1), constraint=constraints.positive) sigma = pyro.sample(&#39;sigma&#39;, dist.TransformedDistribution( dist.Normal(sigma_loc, sigma_scale), transforms=transforms.ExpTransform() )) #sigma = pyro.sample(&#39;sigma&#39;, dist.HalfCauchy(sigma_scale)) . pyro.render_model( pool_sigma_guide, (food_category, customers, sales), render_params=True) . auto_guide = AutoLaplaceApproximation(pooled_sigma_model) . customers = torch.tensor(sales_df[&#39;customers&#39;].values, dtype=torch.float64) sales = torch.tensor(sales_df[&#39;sales&#39;].values, dtype=torch.float64) food_category = torch.tensor(sales_df[&#39;Food_Category&#39;].cat.codes.values, dtype=torch.long) pyro.clear_param_store() adam_params = {&#39;lr&#39;: 0.002} optim = Adam(adam_params) svi = SVI(pooled_sigma_model, auto_guide, optim, Trace_ELBO()) #svi = SVI(pooled_sigma_model, pool_sigma_guide, optim, Trace_ELBO()) iter = 2000 elbo_loss = [] for i in range(iter): loss = svi.step(food_category, customers, sales) elbo_loss.append(loss) plt.figure(figsize=(10, 3)) plt.plot(np.arange(1, iter+1), elbo_loss) plt.ylabel(&#39;ELBO Loss&#39;) plt.xlabel(&#39;Iterations&#39;) plt.title(f&#39;iter {len(elbo_loss)}, loss: {elbo_loss[-1]:.4f}&#39;) plt.show() . predictive = Predictive(pooled_sigma_model, guide=auto_guide, num_samples=1000) posterior_svi_samples = predictive(food_category, customers, None) . sales_df[&#39;food_cat_encode&#39;] = food_category fig, ax = plt.subplots(figsize=(10, 6)) for i in range(3): category_mask = sales_df[&#39;food_cat_encode&#39;] == i mu_cat = posterior_svi_samples[&#39;y&#39;][:, category_mask].mean(axis=0) customers = sales_df.loc[category_mask, [&#39;customers&#39;]].values.flatten() sales = sales_df.loc[category_mask, [&#39;sales&#39;]].values.flatten() ax.plot(customers, mu_cat, c=&#39;black&#39;) ax.scatter(customers, sales) az.plot_hdi( x=customers, y=posterior_svi_samples[&#39;y&#39;][:, category_mask], color=&#39;grey&#39; ) ax.set_xlabel(&#39;Customers&#39;) ax.set_ylabel(&#39;Sales&#39;) ax.set_title(&#39;Pooled Sigma&#39;) . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs) /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs) /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs) . Text(0.5, 1.0, &#39;Pooled Sigma&#39;) . Hierarchical Models . In the multi-level model above, the $ sigma$ is assumed to be the same for all 3 categories. Instead, we can say that $ sigma$ comes from the same underlying distribution, but is allowed to vary by category. . def unpooled_model(food_cat, customers, sales=None): P = 3 N = len(customers) with pyro.plate(&#39;food_cat_i&#39;, len(np.unique(food_cat))): sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(20.)) beta = pyro.sample(&#39;beta&#39;, dist.Normal(10., 10.)) with pyro.plate(&#39;data&#39;, N): mu = pyro.deterministic(&#39;mu&#39;, beta[food_cat] * customers) output = pyro.sample(&#39;y&#39;, dist.Normal(mu, sigma[food_cat]), obs=sales) . def hierarchical_model(food_cat, customers, sales=None): N = len(customers) P = len(np.unique(food_cat)) sigma_hyperprior = pyro.sample(&#39;sigma_hyperprior&#39;, dist.HalfNormal(20.)) with pyro.plate(&#39;food_cat_i&#39;, size=P): beta = pyro.sample(&#39;beta&#39;, dist.Normal(10., 20.)) sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(sigma_hyperprior)) with pyro.plate(&#39;output&#39;, size=N): mu = pyro.deterministic(&#39;mu&#39;, beta[food_cat] * customers) output = pyro.sample(&#39;y&#39;, dist.Normal(mu, sigma[food_cat]), obs=sales) . pyro.render_model( hierarchical_model, (food_category, customers, sales), render_distributions=True ) . kernel = NUTS(hierarchical_model) mcmc_hm = MCMC(kernel, 500, 300) mcmc_hm.run(food_category, customers, sales) . Sample: 100%|██████████| 800/800 [00:14, 55.24it/s, step size=5.92e-01, acc. prob=0.891] . mcmc_hm.summary() . mean std median 5.0% 95.0% n_eff r_hat beta[0] 13.02 0.03 13.02 12.96 13.07 565.98 1.00 beta[1] 8.11 0.22 8.12 7.77 8.45 316.94 1.00 beta[2] 6.11 0.05 6.11 6.03 6.20 765.83 1.00 sigma[0] 40.30 1.58 40.29 37.29 42.55 753.05 1.01 sigma[1] 24.35 11.98 21.33 9.83 39.94 290.73 1.00 sigma[2] 36.09 2.43 35.95 31.60 39.51 628.62 1.00 sigma_hyperprior 31.19 8.84 29.86 17.93 45.08 602.12 1.00 Number of divergences: 0 . Posterior Geometry Matters . def salad_generator(hyperprior_beta_mean=5, hyperprior_beta_sigma=.2, sigma=50, days_per_location=[6, 4, 15, 10, 3, 5], sigma_per_location=[50,10,20,80,30,20]): &quot;&quot;&quot;Generate noisy salad data&quot;&quot;&quot; beta_hyperprior = stats.norm(hyperprior_beta_mean, hyperprior_beta_sigma) # Generate demands days per restaurant df = pd.DataFrame() for i, days in enumerate(days_per_location): np.random.seed(0) num_customers = stats.randint(30, 100).rvs(days) sales_location = beta_hyperprior.rvs()*num_customers + stats.norm(0, sigma_per_location[i]).rvs(num_customers.shape) location_df = pd.DataFrame({&quot;customers&quot;:num_customers, &quot;sales&quot;:sales_location}) location_df[&quot;location&quot;] = i location_df.sort_values(by=&quot;customers&quot;, ascending=True) df = pd.concat([df, location_df]) df.reset_index(inplace=True, drop=True) return df hierarchical_salad_df = salad_generator() . fig, axes, = plt.subplots(2,3, sharex=True, sharey=True, figsize=(18, 8)) for i, ax in enumerate(axes.ravel()): location_filter = (hierarchical_salad_df[&quot;location&quot;] == i) hierarchical_salad_df[location_filter].plot( kind=&quot;scatter&quot;, x=&quot;customers&quot;, y=&quot;sales&quot;, ax=ax) ax.set_xlabel(&quot;&quot;) ax.set_ylabel(&quot;&quot;) axes[1,0].set_xlabel(&quot;Number of Customers&quot;) axes[1,0].set_ylabel(&quot;Sales&quot;); . customers = torch.tensor( hierarchical_salad_df[&#39;customers&#39;].values, dtype=torch.float64) sales = torch.tensor( hierarchical_salad_df[&#39;sales&#39;].values, dtype=torch.float64) location = torch.tensor( hierarchical_salad_df[&#39;location&#39;].values, dtype=torch.long) . def hierarchical_salad_model(customers, location, sales=None): N = len(customers) P = len(np.unique(location)) beta_loc_hyper = pyro.sample(&#39;beta_loc_hyper&#39;, dist.Normal(0., 10.)) beta_scale_hyper = pyro.sample(&#39;beta_scale_hyper&#39;, dist.HalfNormal(.1)) sigma_hyper = pyro.sample(&#39;sigma_hyper&#39;, dist.HalfNormal(30.)) with pyro.plate(&#39;location_i&#39;, size=P): beta = pyro.sample(&#39;beta&#39;, dist.Normal(beta_loc_hyper, beta_scale_hyper)) sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(sigma_hyper)) with pyro.plate(&#39;output&#39;, size=N): mu = pyro.deterministic(&#39;mu&#39;, beta[location] * sigma[location]) output = pyro.sample(&#39;y&#39;, dist.Normal(mu, sigma[location]), obs=sales) . pyro.render_model( hierarchical_salad_model, (customers, location, sales), render_distributions=True ) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_location_i location_i cluster_output output beta_loc_hyper beta_loc_hyper beta beta beta_loc_hyper&#45;&gt;beta beta_scale_hyper beta_scale_hyper beta_scale_hyper&#45;&gt;beta sigma_hyper sigma_hyper sigma sigma sigma_hyper&#45;&gt;sigma y y beta&#45;&gt;y sigma&#45;&gt;y mu mu distribution_description_node beta_loc_hyper ~ Normal beta_scale_hyper ~ HalfNormal sigma_hyper ~ HalfNormal beta ~ Normal sigma ~ HalfNormal mu ~ Delta y ~ Normal kernel = NUTS(hierarchical_salad_model) mcmc_salad = MCMC(kernel, 1000, 300) mcmc_salad.run(customers, location, sales) . Sample: 100%|██████████| 1300/1300 [05:35, 3.87it/s, step size=3.00e-02, acc. prob=0.910] . mcmc_salad.summary() . mean std median 5.0% 95.0% n_eff r_hat beta[0] 3.92 0.42 3.88 3.33 4.69 102.30 1.00 beta[1] 3.93 0.43 3.91 3.28 4.68 104.08 1.00 beta[2] 3.91 0.43 3.89 3.24 4.64 102.43 1.00 beta[3] 3.89 0.42 3.87 3.25 4.62 106.42 1.00 beta[4] 3.93 0.43 3.90 3.31 4.70 98.84 1.00 beta[5] 3.94 0.43 3.91 3.30 4.69 99.00 1.00 beta_loc_hyper 3.92 0.42 3.89 3.29 4.65 97.70 1.00 beta_scale_hyper 0.09 0.06 0.08 0.02 0.19 98.18 1.00 sigma[0] 95.15 13.28 94.21 74.41 116.10 174.19 1.00 sigma[1] 106.92 17.15 104.46 78.28 131.99 211.43 1.00 sigma[2] 97.62 11.32 97.14 80.24 115.61 132.04 1.00 sigma[3] 102.15 12.95 100.91 81.40 121.67 162.97 1.00 sigma[4] 112.65 20.98 108.34 81.88 143.67 192.40 1.00 sigma[5] 108.86 15.33 107.45 82.53 132.22 205.56 1.00 sigma_hyper 78.31 13.69 77.15 56.33 100.84 282.40 1.00 Number of divergences: 18 . salad_samples = mcmc_salad.get_samples(1000) salad_predictive_samples = Predictive( hierarchical_salad_model, salad_samples)(customers, location, None) az_salad_inf = az.from_pyro( posterior=mcmc_salad, posterior_predictive=salad_predictive_samples) . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/data/io_pyro.py:157: UserWarning: Could not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False warnings.warn( . slope_centered = salad_samples[&#39;beta&#39;][..., 4].numpy().flatten() sigma_centered = salad_samples[&#39;beta_scale_hyper&#39;].numpy().flatten() divergences_centered = np.array(mcmc_salad.diagnostics()[&#39;divergences&#39;][&#39;chain 0&#39;]) . divergent_samples = pd.DataFrame({ &#39;slope&#39;: slope_centered, &#39;sigma&#39;: sigma_centered }) mask = divergent_samples.index.isin(pd.Index(divergences_centered)) divergent_samples[&#39;divergence&#39;] = [1 if booly == True else 0 for booly in mask] ax = sns.jointplot( data=divergent_samples, x=&#39;slope&#39;, y=&#39;sigma&#39;, color=&#39;grey&#39;, hue=&#39;divergence&#39;, palette=&quot;muted&quot;) ax.set_axis_labels(xlabel=&#39;$param: beta_m}$&#39;, ylabel=&#39;hyperprior: $ beta_{ sigma}$&#39;) plt.show() . def non_centered_hierarchical_salad_model(customers, location, sales=None): N = len(customers) P = len(np.unique(location)) beta_loc_hyper = pyro.sample(&#39;beta_loc_hyper&#39;, dist.Normal(0., 10.)) beta_scale_hyper = pyro.sample(&#39;beta_scale_hyper&#39;, dist.HalfNormal(.1)) sigma_hyper = pyro.sample(&#39;sigma_hyper&#39;, dist.HalfNormal(30.)) with pyro.plate(&#39;location_i&#39;, size=P): beta_offset = pyro.sample(&#39;beta_offset&#39;, dist.Normal(0., 1.)) #beta = pyro.sample(&#39;beta&#39;, dist.Normal(beta_loc_hyper, beta_scale_hyper)) sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(sigma_hyper)) beta = pyro.deterministic(&#39;beta&#39;, beta_offset * beta_scale_hyper + beta_loc_hyper) with pyro.plate(&#39;output&#39;, size=N): mu = pyro.deterministic(&#39;mu&#39;, beta[location] * sigma[location]) output = pyro.sample(&#39;y&#39;, dist.Normal(mu, sigma[location]), obs=sales) . pyro.render_model( non_centered_hierarchical_salad_model, (customers, location, sales), render_distributions=True ) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_location_i location_i cluster_output output beta_loc_hyper beta_loc_hyper y y beta_loc_hyper&#45;&gt;y beta_scale_hyper beta_scale_hyper beta_scale_hyper&#45;&gt;y sigma_hyper sigma_hyper sigma sigma sigma_hyper&#45;&gt;sigma beta_offset beta_offset beta_offset&#45;&gt;y sigma&#45;&gt;y beta beta mu mu distribution_description_node beta_loc_hyper ~ Normal beta_scale_hyper ~ HalfNormal sigma_hyper ~ HalfNormal beta_offset ~ Normal sigma ~ HalfNormal beta ~ Delta mu ~ Delta y ~ Normal kernel = NUTS(non_centered_hierarchical_salad_model) mcmc_non_centered_salad = MCMC(kernel, 1000, 300) mcmc_non_centered_salad.run(customers, location, sales) . Sample: 100%|██████████| 1300/1300 [01:08, 18.85it/s, step size=2.93e-01, acc. prob=0.925] . mcmc_non_centered_salad.summary() . mean std median 5.0% 95.0% n_eff r_hat beta_loc_hyper 3.94 0.44 3.94 3.22 4.65 377.00 1.00 beta_offset[0] 0.04 0.98 0.07 -1.63 1.60 1540.04 1.00 beta_offset[1] 0.08 0.98 0.08 -1.55 1.69 1184.74 1.00 beta_offset[2] 0.00 0.99 0.00 -1.59 1.58 1204.12 1.00 beta_offset[3] -0.17 0.97 -0.15 -1.62 1.57 1426.89 1.00 beta_offset[4] 0.06 0.98 0.08 -1.74 1.55 1726.16 1.00 beta_offset[5] 0.02 0.91 0.02 -1.45 1.41 1230.93 1.00 beta_scale_hyper 0.08 0.06 0.07 0.00 0.17 901.34 1.00 sigma[0] 95.56 14.36 93.67 73.46 118.34 519.08 1.00 sigma[1] 106.76 16.90 104.28 80.09 130.09 530.26 1.00 sigma[2] 97.61 11.82 96.72 80.04 117.16 444.60 1.00 sigma[3] 102.84 13.34 101.48 82.26 122.98 417.32 1.00 sigma[4] 111.38 19.02 109.00 83.23 141.04 518.78 1.00 sigma[5] 109.15 16.31 107.26 81.99 132.45 602.31 1.00 sigma_hyper 76.33 13.67 74.52 54.63 97.47 1318.93 1.00 Number of divergences: 0 . non_centered_salad_samples = mcmc_non_centered_salad.get_samples(1000) salad_predictive_samples = Predictive( non_centered_hierarchical_salad_model, non_centered_salad_samples)(customers, location, None) az_salad_inf = az.from_pyro( posterior=mcmc_non_centered_salad, posterior_predictive=salad_predictive_samples) . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/data/io_pyro.py:157: UserWarning: Could not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False warnings.warn( . slope_un_centered = salad_predictive_samples[&#39;beta&#39;][..., 4].numpy().flatten() ## index 4th beta param sigma_un_centered = non_centered_salad_samples[&#39;beta_scale_hyper&#39;].numpy().flatten() divergences_un_centered = np.array(mcmc_non_centered_salad.diagnostics()[&#39;divergences&#39;][&#39;chain 0&#39;]) . non_centered = pd.DataFrame({ &#39;beta_b4&#39;: slope_un_centered, &#39;beta_sigma_hyper&#39;: sigma_un_centered, &#39;parameterization&#39;: &#39;non_centered&#39; }) non_centered_mask = non_centered.index.isin(pd.Index(divergences_un_centered)) non_centered[&#39;divergence&#39;] = [1 if booly == True else 0 for booly in non_centered_mask] centered = pd.DataFrame({ &#39;beta_b4&#39;: slope_centered, &#39;beta_sigma_hyper&#39;: sigma_centered, &#39;parameterization&#39;: &#39;centered&#39; }) centered_mask = centered.index.isin(pd.Index(divergences_centered)) centered[&#39;divergence&#39;] = [1 if booly == True else 0 for booly in centered_mask] df = pd.concat([non_centered, centered]) . g = sns.FacetGrid(df, col=&#39;parameterization&#39;, hue=&#39;divergence&#39;, height=5) g.map_dataframe(sns.scatterplot, &#39;beta_b4&#39;, &#39;beta_sigma_hyper&#39;) g.set_axis_labels(xlabel=&#39;$ beta_[4]$&#39;, ylabel=&#39;$ beta_{ sigma h}$&#39;) g.add_legend() plt.show() . plt.figure(figsize=(10, 6)) sns.kdeplot(df[df[&#39;parameterization&#39;] == &#39;centered&#39;][&#39;beta_sigma_hyper&#39;]) sns.kdeplot(df[df[&#39;parameterization&#39;] == &#39;non_centered&#39;][&#39;beta_sigma_hyper&#39;]) plt.legend([&#39;centered&#39;, &#39;non centered&#39;]) plt.title(&#39;Difference in $ beta_{sigma}$ hyperprior estimates&#39;) plt.show() . Predictions at Multiple Levels . Using the fitted parameter estimates to make an out of sample prediction for the distribution of sales for 50 customers. . posterior predictive: learned parameters; the data the model expects to see | customer = data $x$ | . # 6 b/c of 6 locations beta = ( non_centered_salad_samples[&#39;beta_offset&#39;] * non_centered_salad_samples[&#39;beta_scale_hyper&#39;].reshape(-1, 1) + non_centered_salad_samples[&#39;beta_loc_hyper&#39;].reshape(-1, 1) ) beta.size() . torch.Size([1000, 6]) . beta_group = dist.Normal( non_centered_salad_samples[&#39;beta_loc_hyper&#39;], non_centered_salad_samples[&#39;beta_scale_hyper&#39;] ).sample((100,)) # aggregate predictions group_level_sales_prediction = dist.Normal( beta_group * 50, non_centered_salad_samples[&#39;sigma_hyper&#39;] ).sample((100,)) # location 2 and 4 location_two = dist.Normal( beta[:, 2] * 50, non_centered_salad_samples[&#39;sigma&#39;][:, 2] ).sample((100,)) location_four = dist.Normal( beta[:, 4] * 50, non_centered_salad_samples[&#39;sigma&#39;][:, 4] ).sample((100,)) . plt.figure(figsize=(10, 6)) sns.kdeplot(group_level_sales_prediction.flatten(), clip=[0, 600]) sns.kdeplot(location_two.flatten(), clip=[0, 600]) sns.kdeplot(location_four.flatten(), clip=[0, 600]) plt.legend([&#39;All Locations&#39;, &#39;Location 2&#39;, &#39;Location 4&#39;]) plt.xlabel(&#39;Salad Sales&#39;) plt.title(&#39;Predicted Revenue with 50 Customers&#39;) plt.show() .",
            "url": "https://gstechschulte.github.io/cached-projects/jupyter/2022/07/22/bmcp-ch-4.html",
            "relUrl": "/jupyter/2022/07/22/bmcp-ch-4.html",
            "date": " • Jul 22, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Bayesian Modeling and Computation in Pyro - Chapter 3",
            "content": "import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np import arviz as az import torch import pyro import pyro.distributions as dist from pyro.distributions import constraints from pyro.infer import Predictive, TracePredictive, NUTS, MCMC from pyro.infer.autoguide import AutoLaplaceApproximation from pyro.infer import SVI, Trace_ELBO from pyro.optim import Adam from pyro.infer.mcmc.util import summary from palmerpenguins import load_penguins plt.style.use(&#39;ggplot&#39;) . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html from .autonotebook import tqdm as notebook_tqdm . penguins = load_penguins() penguins.dropna(how=&#39;any&#39;, axis=0, inplace=True) . penguins . species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year . 0 Adelie | Torgersen | 39.1 | 18.7 | 181.0 | 3750.0 | male | 2007 | . 1 Adelie | Torgersen | 39.5 | 17.4 | 186.0 | 3800.0 | female | 2007 | . 2 Adelie | Torgersen | 40.3 | 18.0 | 195.0 | 3250.0 | female | 2007 | . 4 Adelie | Torgersen | 36.7 | 19.3 | 193.0 | 3450.0 | female | 2007 | . 5 Adelie | Torgersen | 39.3 | 20.6 | 190.0 | 3650.0 | male | 2007 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 339 Chinstrap | Dream | 55.8 | 19.8 | 207.0 | 4000.0 | male | 2009 | . 340 Chinstrap | Dream | 43.5 | 18.1 | 202.0 | 3400.0 | female | 2009 | . 341 Chinstrap | Dream | 49.6 | 18.2 | 193.0 | 3775.0 | male | 2009 | . 342 Chinstrap | Dream | 50.8 | 19.0 | 210.0 | 4100.0 | male | 2009 | . 343 Chinstrap | Dream | 50.2 | 18.7 | 198.0 | 3775.0 | female | 2009 | . 333 rows × 8 columns . adelie_mask = (penguins[&#39;species&#39;] == &#39;Adelie&#39;) adelie_mass_obs = torch.from_numpy(penguins.loc[adelie_mask, &#39;body_mass_g&#39;].values) . Code 3.3 . def model_prior(obs=None): # priors over params. sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(2000.)) mu = pyro.sample(&#39;mu&#39;, dist.Normal(4000, 3000)) mass = pyro.sample(&#39;mass&#39;, dist.Normal(mu, sigma), obs=obs) . samples = Predictive( model_prior, {}, num_samples=1000, return_sites=[&#39;sigma&#39;, &#39;mu&#39;, &#39;mass&#39;])(adelie_mass_obs) . az.plot_density(data=samples[&#39;mass&#39;].numpy()) . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/data/base.py:220: UserWarning: More chains (1000) than draws (146). Passed array should have shape (chains, draws, *shape) warnings.warn( . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fd16fcf8220&gt;]], dtype=object) . az.plot_density(data=samples[&#39;mu&#39;].numpy()) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fd16fbc6820&gt;]], dtype=object) . az.plot_density(data=samples[&#39;sigma&#39;].numpy()) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fd17045bd30&gt;]], dtype=object) . def model(obs=None): # priors over params. sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(2000.)) mu = pyro.sample(&#39;mu&#39;, dist.Normal(4000, 3000)) mass = pyro.sample(&#39;mass&#39;, dist.Normal(mu, sigma), obs=obs) return mass . kernel = NUTS(model) mcmc = MCMC(kernel, num_samples=500, warmup_steps=300) mcmc.run(obs=adelie_mass_obs) . Sample: 100%|██████████| 800/800 [00:35, 22.47it/s, step size=9.31e-01, acc. prob=0.900] . mcmc.summary() . mean std median 5.0% 95.0% n_eff r_hat mu 3705.38 35.76 3704.62 3653.22 3764.10 479.10 1.00 sigma 462.39 28.02 461.67 413.43 505.38 415.20 1.00 Number of divergences: 0 . samples_1 = mcmc.get_samples() . samples_1 . {&#39;mu&#39;: tensor([3730.0934, 3713.2198, 3764.2792, ..., 3715.3029, 3709.2097, 3747.3848], dtype=torch.float64), &#39;sigma&#39;: tensor([411.5391, 438.3062, 438.7027, ..., 437.0136, 426.0214, 453.0109], dtype=torch.float64)} . 3.2.1 Linear Penguins . adelie_flipper_length = torch.from_numpy(penguins.loc[adelie_mask, &#39;flipper_length_mm&#39;].values) adelie_mass = torch.from_numpy(penguins.loc[adelie_mask, &#39;body_mass_g&#39;].values) . def linear_model(flipper_length, mass=None): sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(2000.)) beta_0 = pyro.sample(&#39;beta_0&#39;, dist.Normal(0., 4000.)) beta_1 = pyro.sample(&#39;beta_1&#39;, dist.Normal(0., 4000.)) mu = pyro.deterministic(&#39;mu&#39;, beta_0 + beta_1 * flipper_length) with pyro.plate(&#39;plate&#39;): preds = pyro.sample(&#39;mass&#39;, dist.Normal(mu, sigma), obs=mass) . pyro.render_model( linear_model, model_args=(adelie_flipper_length, adelie_mass), render_distributions=True ) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_plate plate sigma sigma mass mass sigma&#45;&gt;mass beta_0 beta_0 beta_0&#45;&gt;mass beta_1 beta_1 beta_1&#45;&gt;mass mu mu distribution_description_node sigma ~ HalfNormal beta_0 ~ Normal beta_1 ~ Normal mu ~ Delta mass ~ Normal MCMC . mcmc = fitted MCMC object | mcmc_samples = only the latent variables are returned via the .get_samples() method mu is not returned in the mcmc.get_samples() dict | deterministic parts are only available via the Predictive function. Similarily, observed samples is only available via this function too | . | . def summary(samples): site_stats = {} for site_name, values in samples.items(): marginal_site = pd.DataFrame(values) describe = marginal_site.describe(percentiles=[.05, 0.25, 0.5, 0.75, 0.95]).transpose() site_stats[site_name] = describe[[&quot;mean&quot;, &quot;std&quot;, &quot;5%&quot;, &quot;25%&quot;, &quot;50%&quot;, &quot;75%&quot;, &quot;95%&quot;]] return site_stats . kernel = NUTS(linear_model, adapt_step_size=True) mcmc_simple = MCMC(kernel, num_samples=500, warmup_steps=300) mcmc_simple.run(flipper_length=adelie_flipper_length, mass=adelie_mass) . Sample: 100%|██████████| 800/800 [02:06, 6.32it/s, step size=2.32e-02, acc. prob=0.948] . mcmc_simple.summary() . mean std median 5.0% 95.0% n_eff r_hat beta_0 -2441.18 930.96 -2449.14 -3991.38 -986.24 166.43 1.00 beta_1 32.34 4.90 32.25 24.53 40.44 167.01 1.00 sigma 392.16 20.51 390.22 355.86 422.31 229.88 1.01 Number of divergences: 0 . az.plot_trace(az.from_pyro(mcmc_simple)) plt.tight_layout() . Posterior Predictive Distribution . Sample mass and mu from posterior. . mcmc_samples = mcmc_simple.get_samples(num_samples=1000) # posterior samples predictive = Predictive(linear_model, mcmc_samples) # latent variables: predictive_samples = predictive(flipper_length=adelie_flipper_length, mass=None) for k, v in predictive_samples.items(): print(f&#39;{k}: {tuple(v.shape)}&#39;) . mass: (1000, 146) mu: (1000, 1, 146) . def mcmc_fit(predictive): mass = predictive[&#39;mass&#39;] mass_mu = mass.mean(axis=0) mass_std = mass.std(axis=0) mass_df = pd.DataFrame({ &#39;feat&#39;: adelie_flipper_length, &#39;mean&#39;: mass_mu, &#39;high&#39;: mass_mu + mass_std, &#39;low&#39;: mass_mu - mass_std} ) return mass_df.sort_values(by=[&#39;feat&#39;]) . mass_df = mcmc_fit(predictive=predictive_samples) . plt.figure(figsize=(10, 6)) plt.scatter(adelie_flipper_length.numpy(), adelie_mass.numpy(), alpha=0.5) plt.plot(mass_df[&#39;feat&#39;], mass_df[&#39;mean&#39;], color=&#39;black&#39;) plt.fill_between( mass_df[&#39;feat&#39;], mass_df[&#39;high&#39;], mass_df[&#39;low&#39;], alpha=0.2, color=&#39;grey&#39;) plt.xlabel(&#39;adelie_flipper_length&#39;) plt.ylabel(&#39;mass&#39;) plt.title(&#39;$ mu = beta_0 + beta_1X_1$&#39;) plt.show() . SVI . Using SVI for a simple linear regression like this is probably overkill, but lets do it anyways. . For the surrogate, we use the predefined AutoNormal . NOT WORKING: I believe it has to do with the autoguide. Solution could be to implement the guide by hand . pyro.get_param_store() is comprised of learned parameters that will be used in the Predictive stage. Instead of providing samples, the guide parameter is used to construct the posterior predictive distribution . 3.3 Multiple Linear Regression . sex_obs = torch.from_numpy(penguins.loc[adelie_mask, &#39;sex&#39;].replace({&#39;male&#39;: 0, &#39;female&#39;: 1}).values) plt.figure(figsize=(10, 6)) sns.scatterplot( x=adelie_flipper_length, y=adelie_mass_obs, hue=sex_obs, alpha=0.5) plt.xlabel(&#39;adelie_flipper_length&#39;) plt.ylabel(&#39;mass&#39;) plt.show() . def linear_model(flipper_length, sex, mass=None): sigma = pyro.sample(&#39;sigma&#39;, dist.HalfNormal(2000.)) beta_0 = pyro.sample(&#39;beta_0&#39;, dist.Normal(0., 3000.)) beta_1 = pyro.sample(&#39;beta_1&#39;, dist.Normal(0., 3000.)) beta_2 = pyro.sample(&#39;beta_2&#39;, dist.Normal(0., 3000.)) mu = pyro.deterministic(&#39;mu&#39;, beta_0 + beta_1 * flipper_length + beta_2 * sex) with pyro.plate(&#39;plate&#39;): preds = pyro.sample(&#39;mass&#39;, dist.Normal(mu, sigma), obs=mass) . pyro.render_model( linear_model, model_args=(adelie_flipper_length, sex_obs, adelie_mass), render_distributions=True ) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_plate plate sigma sigma mass mass sigma&#45;&gt;mass beta_0 beta_0 beta_0&#45;&gt;mass beta_1 beta_1 beta_1&#45;&gt;mass beta_2 beta_2 beta_2&#45;&gt;mass mu mu distribution_description_node sigma ~ HalfNormal beta_0 ~ Normal beta_1 ~ Normal beta_2 ~ Normal mu ~ Delta mass ~ Normal MCMC . kernel = NUTS(linear_model, adapt_step_size=True) mcmc = MCMC(kernel, num_samples=500, warmup_steps=300, num_chains=1) mcmc.run(flipper_length=adelie_flipper_length, sex=sex_obs, mass=adelie_mass) . Sample: 100%|██████████| 800/800 [02:39, 5.03it/s, step size=2.93e-02, acc. prob=0.940] . mcmc.summary() . mean std median 5.0% 95.0% n_eff r_hat beta_0 858.30 705.14 822.31 -317.98 1788.14 141.47 1.00 beta_1 16.55 3.66 16.76 11.60 22.51 142.56 1.00 beta_2 -596.87 47.02 -596.83 -674.49 -519.23 362.03 1.00 sigma 288.38 16.31 286.80 261.90 313.87 301.37 1.00 Number of divergences: 0 . az.plot_trace(az.from_pyro(mcmc)) plt.tight_layout() . mcmc_samples = mcmc.get_samples(num_samples=1000) # posterior samples predictive = Predictive(linear_model, mcmc_samples) # posterior predictive predictive_samples = predictive(flipper_length=adelie_flipper_length, sex=sex_obs, mass=None) for k, v in predictive_samples.items(): print(f&#39;{k}: {tuple(v.shape)}&#39;) . mass: (1000, 146) mu: (1000, 1, 146) . mass_mu = predictive_samples[&#39;mass&#39;].numpy().mean(axis=0) mass_std = predictive_samples[&#39;mass&#39;].numpy().std(axis=0) predictions = pd.DataFrame({ &#39;sex&#39;: sex_obs, &#39;flipper&#39;: adelie_flipper_length, &#39;mass_mu&#39;: mass_mu, &#39;mass_std&#39;: mass_std, &#39;high&#39;: mass_mu + mass_std, &#39;low&#39;: mass_mu - mass_std }) predictions = predictions.sort_values(by=[&#39;flipper&#39;]) . male = predictions[predictions[&#39;sex&#39;] == 0] female = predictions[predictions[&#39;sex&#39;] == 1] . plt.figure(figsize=(10, 6)) sns.scatterplot( x=adelie_flipper_length, y=adelie_mass_obs, hue=sex_obs, alpha=0.5) plt.plot(male[&#39;flipper&#39;], male[&#39;mass_mu&#39;]) plt.plot(female[&#39;flipper&#39;], female[&#39;mass_mu&#39;]) plt.fill_between( male[&#39;flipper&#39;], male[&#39;high&#39;], male[&#39;low&#39;], alpha=0.2, color=&#39;grey&#39;) plt.fill_between( female[&#39;flipper&#39;], female[&#39;high&#39;], female[&#39;low&#39;], alpha=0.2, color=&#39;grey&#39;) plt.xlabel(&#39;adelie_flipper_length&#39;) plt.ylabel(&#39;mass&#39;) plt.title(&#39;Predictions $ pm 1 sigma$ &#39;) plt.show() . mcmc_multiple_az = az.from_pyro(mcmc) mcmc_simple_az = az.from_pyro(mcmc_simple) az.plot_forest([mcmc_simple_az, mcmc_multiple_az], var_names=[&#39;sigma&#39;]) # manually specify to avoid confusion plt.legend([&#39;flipper_x_sex $ sigma$&#39;, &#39;flipper_only $ sigma$&#39;]) plt.show() . Counterfactuals . Generalized Linear Models . Logistic Regression . species_filter = penguins[&#39;species&#39;].isin([&#39;Adelie&#39;, &#39;Chinstrap&#39;]) bill_length_obs = torch.from_numpy(penguins.loc[species_filter, &#39;bill_length_mm&#39;].values.reshape(-1, 1)) bill_length_obs = torch.tensor(bill_length_obs, dtype=torch.float) species = pd.Categorical(penguins.loc[species_filter, &#39;species&#39;]) species_codes = torch.from_numpy(species.codes).to(torch.float64) species_codes = torch.tensor(species_codes, dtype=torch.float) . def logistic_model(bill_length, species=None): N, P = bill_length.shape beta_0 = pyro.sample(&#39;beta_0&#39;, dist.Normal(0., 10.)) beta_1 = pyro.sample(&#39;beta_1&#39;, dist.Normal(0., 10.).expand([P])) mu = beta_0 + torch.matmul(beta_1, bill_length.T) theta = pyro.deterministic(&#39;theta&#39;, torch.sigmoid(mu)) db = pyro.deterministic(&#39;db&#39;, -beta_0 / beta_1) with pyro.plate(&#39;plate&#39;): y1 = pyro.sample(&#39;y1&#39;, dist.Bernoulli(theta), obs=species) . pyro.render_model( logistic_model, model_args=(bill_length_obs, species_codes), render_distributions=True) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_plate plate beta_0 beta_0 y1 y1 beta_0&#45;&gt;y1 beta_1 beta_1 beta_1&#45;&gt;y1 theta theta db db distribution_description_node beta_0 ~ Normal beta_1 ~ Normal theta ~ Delta db ~ Delta y1 ~ Bernoulli MCMC . kernel = NUTS(logistic_model, adapt_step_size=True) mcmc_logistic = MCMC(kernel, num_samples=500, warmup_steps=300) mcmc_logistic.run(bill_length=bill_length_obs, species=species_codes) . Sample: 100%|██████████| 800/800 [00:40, 19.70it/s, step size=2.26e-02, acc. prob=0.930] . mcmc_logistic.summary() . mean std median 5.0% 95.0% n_eff r_hat beta_0 -35.22 4.74 -35.19 -42.34 -27.55 85.09 1.03 beta_1[0] 0.80 0.11 0.80 0.61 0.95 84.65 1.03 Number of divergences: 0 . az.plot_trace(az.from_pyro(mcmc_logistic)) plt.tight_layout() . mcmc_samples = mcmc_logistic.get_samples(num_samples=1000) # posterior samples predictive = Predictive(logistic_model, mcmc_samples) # posterior predictive predictive_samples = predictive(bill_length_obs, None) for k, v in predictive_samples.items(): print(f&#39;{k}: {tuple(v.shape)}&#39;) . y1: (1000, 214) theta: (1000, 1, 214) db: (1000, 1, 1) . prob_mu = predictive_samples[&#39;theta&#39;].numpy().mean(axis=0).flatten() prob_std = predictive_samples[&#39;theta&#39;].numpy().std(axis=0).flatten() db_mu = predictive_samples[&#39;db&#39;].numpy().mean() db_std = predictive_samples[&#39;db&#39;].numpy().std() predictions = pd.DataFrame({ &#39;bill_length&#39;: bill_length_obs.flatten(), &#39;prob_mu&#39;: prob_mu, &#39;prob_std&#39;: prob_std, &#39;high&#39;: prob_mu + prob_std, &#39;low&#39;: prob_mu - prob_std }) predictions = predictions.sort_values(by=[&#39;bill_length&#39;]) . plt.figure(figsize=(10, 6)) for i, (label, marker) in enumerate(zip(species.categories, (&quot;.&quot;, &quot;s&quot;))): _filter = (species.codes == i) ## size x = bill_length_obs[_filter] ## x_obs y = np.random.normal(i, 0.02, size=_filter.sum()) ## small amount of noise (jitter) plt.scatter(bill_length_obs[_filter], y, marker=marker, label=label, alpha=.8) plt.plot(predictions[&#39;bill_length&#39;], predictions[&#39;prob_mu&#39;], color=&#39;black&#39;) plt.fill_between( predictions[&#39;bill_length&#39;], predictions[&#39;high&#39;], predictions[&#39;low&#39;], alpha=0.25, color=&#39;grey&#39;) plt.axvline( x=predictive_samples[&#39;db&#39;].numpy().mean(), linestyle=&#39;--&#39;, color=&#39;black&#39;) plt.xlabel(&#39;bill length&#39;) plt.ylabel(&#39;Probability&#39;) plt.title(&#39;Logistic Model - MCMC&#39;) plt.show() . SVI . def logistic_guide(bill_length, species=None): N, P = bill_length.shape beta_0_loc = pyro.param(&#39;beta_0_loc&#39;, torch.tensor(0.)) beta_0_scale = pyro.param(&#39;beta_0_scale&#39;, torch.tensor(0.1), constraint=constraints.positive) beta_0 = pyro.sample(&#39;beta_0&#39;, dist.Normal(beta_0_loc, beta_0_scale)) beta_1_loc = pyro.param(&#39;beta_1_loc&#39;, torch.tensor(0.1)) beta_1_scale = pyro.param(&#39;beta_1_scale&#39;, torch.tensor(0.1), constraint=constraints.positive) beta_1 = pyro.sample(&#39;beta_1&#39;, dist.Normal(beta_1_loc, beta_1_scale).expand([P])) . pyro.render_model( logistic_guide, model_args=(bill_length_obs, species_codes), render_params=True ) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 beta_0 beta_0 beta_1 beta_1 beta_1_loc beta_1_loc beta_1_loc&#45;&gt;beta_1 beta_0_loc beta_0_loc beta_0_loc&#45;&gt;beta_0 beta_1_scale beta_1_scale beta_1_scale&#45;&gt;beta_1 beta_0_scale beta_0_scale beta_0_scale&#45;&gt;beta_0 pyro.clear_param_store() optim = Adam({&quot;lr&quot;: 0.3}) svi = SVI(logistic_model, logistic_guide, optim, Trace_ELBO()) elbo_loss = [] for i in range(1000): loss = svi.step(bill_length_obs, species_codes) elbo_loss.append(loss) . plt.figure(figsize=(10, 3)) plt.plot(np.arange(1, 1001), elbo_loss) plt.ylabel(&#39;ELBO Loss&#39;) plt.xlabel(&#39;Iterations&#39;) plt.title(f&#39;iter: 1000, loss: {elbo_loss[-1]:.4f}&#39;) plt.show() . for name, value in pyro.get_param_store().items(): print(name, pyro.param(name)) . beta_0_loc tensor(-24.6493, requires_grad=True) beta_0_scale tensor(0.2914, grad_fn=&lt;AddBackward0&gt;) beta_1_loc tensor(0.5666, requires_grad=True) beta_1_scale tensor(0.0053, grad_fn=&lt;AddBackward0&gt;) . predictive = Predictive(logistic_model, guide=logistic_guide, num_samples=1000) posterior_svi_samples = predictive(bill_length_obs, None) . prob_mu = posterior_svi_samples[&#39;theta&#39;].numpy().mean(axis=0).flatten() prob_std = posterior_svi_samples[&#39;theta&#39;].numpy().std(axis=0).flatten() predictions = pd.DataFrame({ &#39;bill_length&#39;: bill_length_obs.flatten(), &#39;prob_mu&#39;: prob_mu, &#39;prob_std&#39;: prob_std, &#39;high&#39;: prob_mu + prob_std, &#39;low&#39;: prob_mu - prob_std }) predictions = predictions.sort_values(by=[&#39;bill_length&#39;]) . plt.figure(figsize=(10, 6)) for i, (label, marker) in enumerate(zip(species.categories, (&quot;.&quot;, &quot;s&quot;))): _filter = (species.codes == i) ## size x = bill_length_obs[_filter] ## x_obs y = np.random.normal(i, 0.02, size=_filter.sum()) ## add small amount of noise for plotting plt.scatter(bill_length_obs[_filter], y, marker=marker, label=label, alpha=.8) plt.plot(predictions[&#39;bill_length&#39;], predictions[&#39;prob_mu&#39;], color=&#39;black&#39;) plt.fill_between( predictions[&#39;bill_length&#39;], predictions[&#39;high&#39;], predictions[&#39;low&#39;], alpha=0.25, color=&#39;grey&#39;) plt.axvline( x=predictive_samples[&#39;db&#39;].numpy().mean(), linestyle=&#39;--&#39;, color=&#39;black&#39;) plt.xlabel(&#39;bill length&#39;) plt.ylabel(&#39;Probability&#39;) plt.title(&#39;Logistic Model - SVI&#39;) plt.show() . Code 3.23 . Use body mass as a covariate . Code 3.24 . Using body mass and flipper length as covariates . When creating a multidimensional distribution in pyro, there is the added functionality of .to_event(1). This method implies that &quot;these dimensions should be treated as a single event&quot;. . see discussion here | . dist.Normal(0., 20.).expand([2]).to_event(1) . Independent(Normal(loc: torch.Size([2]), scale: torch.Size([2])), 1) . X = penguins.loc[species_filter, [&#39;bill_length_mm&#39;, &#39;body_mass_g&#39;]] X = torch.from_numpy(X.values) species_codes = species_codes.reshape(-1, 1) . def multiple_logistic_model(data_matrix, species=None): N, K = data_matrix.size() beta = pyro.sample(&quot;b&quot;, dist.Normal(0, 10)) w = pyro.sample(&#39;coef&#39;, dist.Normal(0., 20.).expand([K])) mu = beta + torch.mul(data_matrix, w) theta = pyro.deterministic(&#39;theta&#39;, torch.sigmoid(mu)) with pyro.plate(&#39;plate&#39;): y1 = pyro.sample(&#39;obs&#39;, dist.Bernoulli(theta), obs=species) . pyro.render_model( multiple_logistic_model, model_args=(X, species_codes) ) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_plate plate b b obs obs b&#45;&gt;obs coef coef coef&#45;&gt;obs theta theta kernel = NUTS(model=multiple_logistic_model, adapt_step_size=True) mcmc_mult_logistic = MCMC(kernel, num_samples=1200, warmup_steps=500) mcmc_mult_logistic.run(X, species_codes) . Sample: 100%|██████████| 1700/1700 [00:17, 94.46it/s, step size=2.26e-01, acc. prob=0.750] . mcmc_mult_logistic.summary() . mean std median 5.0% 95.0% n_eff r_hat b 0.02 10.20 0.14 -15.60 16.55 119.77 1.00 coef[0] -16.17 11.70 -14.33 -32.86 -0.66 135.29 1.00 coef[1] -18.33 13.70 -15.16 -39.46 -0.04 68.52 1.00 Number of divergences: 1041 . mcmc_mult_samples = mcmc_mult_logistic.get_samples(num_samples=1000) post_predictive = Predictive(multiple_logistic_model, mcmc_mult_samples) predictive_samples = post_predictive(X, None) for k, v in predictive_samples.items(): print(f&#39;{k}: {tuple(v.shape)}&#39;) . obs: (1000, 214, 2) theta: (1000, 1, 214, 2) . inf_data = az.from_pyro( mcmc_mult_logistic, posterior_predictive=mcmc_mult_samples ) . /Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/data/io_pyro.py:157: UserWarning: Could not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False warnings.warn( posterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented. posterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented. . az.plot_trace(inf_data, compact=False, var_names=[&#39;b&#39;, &#39;coef&#39;]) plt.tight_layout() plt.show() . az.summary(inf_data, var_names=[&#39;b&#39;, &#39;coef&#39;]) . arviz - WARNING - Shape validation failed: input_shape: (1, 1200), minimum_shape: (chains=2, draws=4) . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . b 0.018 | 10.196 | -18.098 | 18.778 | 0.933 | 0.661 | 123.0 | 201.0 | NaN | . coef[0] -16.169 | 11.696 | -36.743 | -0.661 | 0.993 | 0.703 | 122.0 | 164.0 | NaN | . coef[1] -18.328 | 13.697 | -44.123 | -0.043 | 1.722 | 1.223 | 61.0 | 101.0 | NaN | . prob_mu = predictive_samples[&#39;theta&#39;].mean(axis=0)[0][:, 0] prob_std = predictive_samples[&#39;theta&#39;].std(axis=0)[0][:, 0] #db_mu = predictive_samples[&#39;db&#39;].numpy().mean() #db_std = predictive_samples[&#39;db&#39;].numpy().std() predictions = pd.DataFrame({ &#39;bill_length&#39;: X[:, 0].numpy(), &#39;mass&#39;: X[:, 1].numpy(), &#39;prob_mu&#39;: prob_mu, &#39;prob_std&#39;: prob_std, &#39;high&#39;: prob_mu + prob_std, &#39;low&#39;: prob_mu - prob_std }) predictions = predictions.sort_values(by=[&#39;bill_length&#39;, &#39;mass&#39;]) . predictions . bill_length mass prob_mu prob_std high low . 136 32.1 | 3050.0 | 3.537150e-16 | 5.096275e-15 | 5.449990e-15 | -4.742560e-15 | . 92 33.1 | 2900.0 | 1.517540e-16 | 2.213480e-15 | 2.365234e-15 | -2.061726e-15 | . 64 33.5 | 3600.0 | 1.082795e-16 | 1.586066e-15 | 1.694345e-15 | -1.477787e-15 | . 86 34.0 | 3400.0 | 7.106088e-17 | 1.045858e-15 | 1.116918e-15 | -9.747967e-16 | . 13 34.4 | 3325.0 | 5.076441e-17 | 7.496711e-16 | 8.004355e-16 | -6.989067e-16 | . ... ... | ... | ... | ... | ... | ... | . 175 52.8 | 4550.0 | 2.214820e-23 | 4.308487e-22 | 4.529969e-22 | -4.087005e-22 | . 185 53.5 | 4500.0 | 1.326296e-23 | 2.669512e-22 | 2.802142e-22 | -2.536883e-22 | . 177 54.2 | 4300.0 | 7.970474e-24 | 1.659051e-22 | 1.738756e-22 | -1.579347e-22 | . 209 55.8 | 4000.0 | 2.520593e-24 | 5.642782e-23 | 5.894841e-23 | -5.390723e-23 | . 163 58.0 | 3700.0 | 5.314031e-25 | 1.297566e-23 | 1.350707e-23 | -1.244426e-23 | . 214 rows × 6 columns .",
            "url": "https://gstechschulte.github.io/cached-projects/jupyter/2022/07/12/bmcp-ch-3.html",
            "relUrl": "/jupyter/2022/07/12/bmcp-ch-3.html",
            "date": " • Jul 12, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Probabilistic Prediction Problems - Part 2",
            "content": "Part two deals with model evaluation and selection using the metrics and scoring rules defined in part one. . The Problem with Parameters . Three main monsters when it comes to “modeling”: . Overfitting | Underfitting | Con-founders | The goal of the model should be stated before you choose your methods to tame these monsters: . Is the goal predictive power? | Is the goal to understand causes? | . Regarding monster (1), adding variables and parameters to a model can help to reveal hidden effects and improve estimates. However, more parameters always results in a better model “fit”. While more complex models fit the data better, they often predict new data worse. Models that have many parameters tend to overfit more than simpler models. Generally, fit is measured by how well the model can retrodict the data used to fit the model. A common metric for this is “variance explained”, $R^2$. Monster (2) hurts, too as underfitting produces models that are inaccurate both within and out of sample. Underfit models have learned too little, whether that be from uninformative features or too simple a model. . So, how to navigate overfitting and underfitting? First, pick a criterion of model performance as the target—what do you want the model to be good at? Methods based on information theory can provide a common and useful target. . Evaluating Generative Models . Generative models aim to model the underlying generative process of the data, typically using Bayes theorem from which we can also generate new samples: . p(z∣x)=p(x∣z)p(z)p(x)p(z | x) = frac{p(x|z)p(z)}{p(x)}p(z∣x)=p(x)p(x∣z)p(z)​ . Generally speaking, when evaluating generative models, we want the metrics to capture: . sample quality - are the samples generated by the model part of the data distribution? | sample diversity - are the samples from the model distribution capturing all modes of the data? | generalization - is the model generalizing beyond the training data? | . Model Selection and Evaluation . To check the results of modeling and inference, we would like to know how well a model fits observed data $x$, which we can quantify with the evidence or marginal likelihood. .",
            "url": "https://gstechschulte.github.io/cached-projects/probability/2022/06/30/Probabilistic-Prediction-Problems-Part-2.html",
            "relUrl": "/probability/2022/06/30/Probabilistic-Prediction-Problems-Part-2.html",
            "date": " • Jun 30, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Variational Inference - ELBO",
            "content": "We don’t know the real posterior so we are going to choose a distribution $Q( theta)$ from a family of distributions $Q^$ that are easy to work with and parameterized by $ theta$. The approximate distribution should be *as close as possible to the true posterior. This closeness is measured using KL-Divergence. If we have the joint $p(x, z)$ where $x$ is some observed data, the goal is to perform inference: given what we have observed, what can we infer about the latent states?, i.e , we want the posterior. . Recall Bayes theorem: . p(z∣x)=p(x∣z)p(z)p(x)p(z | x) = frac{p(x|z)p(z)}{p(x)}p(z∣x)=p(x)p(x∣z)p(z)​ . The problem is the marginal $p(x = D)$ as this could require a hundred, thousand, . . .dimensional integral: . p(x)=∫z0,...,∫zD−1p(x,z)dz0,...,dzD−1p(x) = int_{z_0},..., int_{z_{D-1}}p(x, z)dz_0,...,d_{z_D{-1}}p(x)=∫z0​​,…,∫zD−1​​p(x,z)dz0​,…,dzD​−1​ . If we want the full posterior and can’t compute the marginal, then what’s the solution? Surrogate posterior. We want to approximate the true posterior using some known distribution: . q(z)≈p(z∣X=D)q(z) approx p(z|X=D)q(z)≈p(z∣X=D) . where $ approx$ can mean you want the approximated posterior to be “as good as possible”. Using variational inference, the objective is to minimize the distance between the surrogate $q(z)$ and the true posterior $p(x)$ using KL-Divergence: . q∗(z)=argminq(z)∈Q(KL(q(z)∣∣p(z∣x=D)))q^*(z) = argmin_{q(z) in Q} (KL(q(z) || p(z|x=D)))q∗(z)=argminq(z)∈Q​(KL(q(z)∣∣p(z∣x=D))) . where $Q$ is a more “simple” distribution. We can restate the KL-divergence as the expectation: . KL(q(z)∣∣p(z∣D))=Ez∼q(z)[logq(z)p(z∣D)]KL(q(z) || p(z|D)) = mathbb{E_{z sim q(z)}}[log frac{q(z)}{p(z|D)}]KL(q(z)∣∣p(z∣D))=Ez∼q(z)​[logp(z∣D)q(z)​] . which, taking the expectation over $z$, is equivalent to integration: . ∫z0,...,∫zD−1q(z)logq(z)p(z∣D)dz0,...,dzD−1 int_{z_0}, . . ., int_{z_{D-1}}q(z)log frac{q(z)}{p(z|D)}d_{z_0},...,d_{z_{D-1}}∫z0​​,…,∫zD−1​​q(z)logp(z∣D)q(z)​dz0​​,…,dzD−1​​ . But, sadly we don’t have $p(z vert D)$ as this is the posterior! We only have the joint. Solution? Recall our KL-divergence: . KL(q(z)∣∣p(z∣D))KL(q(z) || p(z|D))KL(q(z)∣∣p(z∣D)) . We can rearrange the terms inside the $log$ so that we can actually compute something: . ∫zq(z)log(q(z)p(D)p(z,D))dz int_{z}q(z)log( frac{q(z)p(D)}{p(z, D)})dz∫z​q(z)log(p(z,D)q(z)p(D)​)dz . We only have the joint. Not the posterior; nor the marginal. We know from Bayes rule that we can express the posterior in terms of the joint $p(z, D)$ divided by the marginal $p(x=D)$: . p(z∣D)=p(Z,D)p(D)p(z|D) = frac{p(Z, D)}{p(D)}p(z∣D)=p(D)p(Z,D)​ . We plug this inside of the $log$: . ∫zq(z)log(q(z)p(D)p(z,D))dz int_{z}q(z)log( frac{q(z)p(D)}{p(z, D)})dz∫z​q(z)log(p(z,D)q(z)p(D)​)dz . However, the problem now is that we have reformulated our problem into another quantity that we don’t have, i.e., the marginal $p(D)$. But we can put the quantity that we don’t have outside of the $log$ to form two separate integrals. . ∫zq(z)log(q(z)p(z,D))dz+∫zq(z)log(p(D)dz int_z q(z)log( frac{q(z)}{p(z, D)})dz + int_zq(z)log(p(D)dz∫z​q(z)log(p(z,D)q(z)​)dz+∫z​q(z)log(p(D)dz . This is a valid rearrangement because of the properties of logarithms. In this case, the numerator is a product, so this turns into a sum of the second integral. What do we see in these two terms? We see an expectation over the quantity $ frac{q(z)}{p(z, D)}$ and another expectation over $p(D)$. Rewriting in terms of expectation: . Ez∼q(z)[log(q(z)p(z,D))]+Ez∼q(z)[log(p(D))] mathbb{E_{z{ sim q(z)}}}[log( frac{q(z)}{p(z, D)})] + mathbb{E_{z sim q(z)}}[log(p(D))]Ez∼q(z)​[log(p(z,D)q(z)​)]+Ez∼q(z)​[log(p(D))] . The right term contains information we know—the functional form of the surrogate $q(z)$ and the joint $p(z, D)$ (in the form of a directed graphical model). We still don’t have access to $p(D)$ on the right side, but this is a constant quantity. The expectation of a quantity that does not contain $z$ is just whatever the expectation was taken over. Because of this, we can again rearrange: . −Ez∼q(z)[logp(z,D)q(z)]+log(p(D))- mathbb{E_{z sim q(z)}}[log frac{p(z, D)}{q(z)}]+log (p(D))−Ez∼q(z)​[logq(z)p(z,D)​]+log(p(D)) . The minus sign is a result of the “swapping” of the numerator and denominator and is required to make it a valid change. Looking at this, the left side is a function dependent on $q$. In shorthand form, we can call this $ mathcal{L(q)}$. Our KL-divergence is: . KL=−L(q)+log(p(D))⏟evidenceKL = mathcal{-L(q)} + underbrace{log(p(D))}_ textrm{evidence}KL=−L(q)+evidence . log(p(D))​​ . where $p(D)$ is a value between $[0, 1]$ and this value is called the evidence which is the log probability of the data. If we apply the $log$ to something between $[0, 1]$ then this value will be negative. This value is also constant since we have observed the dataset and thus does not change. . $KL$ is the distance (between the posterior and the surrogate) so it must be something positive. If the $KL$ is positive and the evidence is negative, then in order to fulfill this equation, $ mathcal{L}$ must also be negative (negative times a negative is a positive). The $ mathcal{L}$ should be smaller than the evidence, and thus it is called the lower bound of the evidence $ rightarrow$ Evidence Lower Bound (ELBO). . Again, ELBO is defined as: $ mathcal{L} = mathbb{E_{z sim q(z)}}[log( frac{p(z, D)}{q(z)})]$ and is important to note that the ELBO is equal to the evidence if and only if the KL-divergence between the surrogate and the true posterior is $0$: . L(q)=log(p(D)) i.f.f. KL(q(z)∣∣p(z∣D))=0 mathcal{L(q)} = log(p(D)) textrm{ i.f.f. } KL(q(z)||p(z|D))=0L(q)=log(p(D)) i.f.f. KL(q(z)∣∣p(z∣D))=0 .",
            "url": "https://gstechschulte.github.io/cached-projects/probability/optimization/2022/06/23/ELBO.html",
            "relUrl": "/probability/optimization/2022/06/23/ELBO.html",
            "date": " • Jun 23, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "One is a Crowd",
            "content": "In the book, Noise: A Flaw in Human Judgement, the authors point out a study conducted by Edward Val and Harold Pashler in which the subjects answer a question, and then subsequently, are asked to answer the same question again. The subjects did not know they would have to answer the question twice. Val and Pashler’s hypothesis was that the average of the two answers would be more accurate than either of the two answers on their own. The researchers who conducted the study drew inspiration from the wisdom of the crowds effect—averaging independent judgements of different people generally improves accuracy. . We do not always produce identical judgements when faced with the same facts on different occasions. .",
            "url": "https://gstechschulte.github.io/cached-projects/probability/reading/2022/06/21/Judgements-Probability.html",
            "relUrl": "/probability/reading/2022/06/21/Judgements-Probability.html",
            "date": " • Jun 21, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Probabilistic Prediction Problems - Part 1",
            "content": "Part one is a collection of notes on the problems with probabilistic predictions. . Distributions over Actions . It is possible to assume the set of possible actions is to pick a class label (or “reject” option) in classification or a real valued scalar as in a point estimate of a parameter. However, it is also possible to assume the set of possible actions is to pick a probability distribution over some value of interest (paramter or class label). That is, we want to perform probabilistic prediction or probabilistic forecasting rather than predicting a scalar. From a decision theoretic approach, we assume the true state of nature is a distribution, $h = p(Y vert x)$, with the action being another distribution, $a = q(Y vert x)$. The goal is to be as close to the true state of nature $p$ with our approximation $q$. Therefore, we want to pick $q$ to minimize $ mathbb{E} ell(p, q)$ for a given $x$. . When making a prediction, the accuracy depends upon the definition of the target, and there is no universal best target. Rather, a decision-theoretic approach should be taken, in particular because of the following two dimensions: . Cost benefit analysis - What’s the risk (or cost) we face when we are wrong? What’s the payoff when we are right? | Context - Some prediction tasks are inheritently harder than others. Therefore, how can we judge how much a model can possibly improve prediction? | KL Divergence, Cross-Entropy, and Log-Loss . If we want to compare two distributions, a common loss function is Kullback-Leibler Divergence (KL Divergence). . Proper Scoring Rules . The key property of proper scoring rules is that the loss function is minimized i.f.f the decision maker picks the distribution $q$ that matches the true distribution $p$. Such a loss function $ ell$ is a proper scoring rule. . Maximizing a proper scoring rule will force the model to match the true probabilities, i.e., the probabilities are calibrated. For example, when the weather person states the probability of rain is $70 %$, then it should rain about $70 %$ of the time. . The following are proper scoring “loss functions”: . Negative log-likelihood - negative of the log-likelihood because maximization is for losers log scoring rule - log of the joint probability | . | Cross entropy - log-loss | . | Brier score - | . Information and Uncertainty . We want to use the log probability of the data to score the accuracy of competing models .",
            "url": "https://gstechschulte.github.io/cached-projects/probability/2022/05/31/Probabilistic-Predicion-Problems-Part-1.html",
            "relUrl": "/probability/2022/05/31/Probabilistic-Predicion-Problems-Part-1.html",
            "date": " • May 31, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Textbooks Have Gotten Good, Like Really Good",
            "content": "In a recent interview, they presented the problem they were facing and asked about the methods I would use to go about solving it. I started off with the heuristics behind the method (Bayesian inference if you are curious) and continued into some of the more detailed advantages and disadvantages behind why I would choose this method. I think they were a bit surprised about this as my original background is in economics, which is traditionally a domain that employs frequentist based methodologies. This led the interviewers to asking, “It seems you are quite familiar with Bayesian statistics and given your background, how did you come to using Bayesian methods within your projects?”. To which I responded, “I first read about the concept in Thinking Fast and Slow by Daniel Kahneman and became deeply intrigued, so I read a few textbooks about Bayesian statistics and Bayesian methods in machine learning.” The interviewers responses, “You read textbooks?”. . Initially, I was a little confused as their response inclined that reading technical textbooks is something that is not normal and or only the advanced have the capabilities to do. So I started to ask myself “How am I able to learn from such technical textbooks?” and started to swipe through the pdf textbooks from various authors and publishing dates. I came to the realization that the textbooks I most often read or refer back to are the ones that were recently published. This observation lead to the conclusion that modern textbooks are, well, very modern and interactive—directional linking throughout sections, hyperlinks to GitHub repositories and Google Colab notebooks for reproducible code, and excellent visualizations of the concepts presented throughout that chapter. . The improvement in technical textbooks is really a result of three ingredients: . 1.) Utilizing Jupyter Notebooks / Google Colab | 2.) Incorporating GitHub public repositories | 3.) The author(s) and or community providing code for the chapter’s content | . Kevin Murphy’s latest release “Probabilistic Machine Learning - An Introduction” has it’s own GitHub repository with Google Colab notebooks for generating figures for each chapter presented in the book. Not only that, but also supplementary material links for each chapter with additional information, typically Google Colab notebooks, showcasing the software libraries used to generate the figures. . When these three ingredients are brought together, you now have an interactive textbook in which some of the barriers to entry, in the form of prior knowledge and skillsets, are broken down and allow for a wider audience to gain said new knowledge and skillsets. Furthermore, and where traditional non-interactive textbooks lack, an interactive textbook’s main advantages are in the form of trial and error, math formalisms $ rightarrow$ code, and indirect efficient programming principles. . The mysterious power of trial and error could be the biggest advantage. Simply playing around with the code, inputing different values here and there to see how the outputs are affected allows one to gain a level of intuition that is not as easily accessible with textbooks without code or visualizations. Try something. It might not work and if it doesn’t, try something else—an error is just another opportunity to run another trial. . After having read the definitions and formalisms, going from math $ rightarrow$ code can be a difficult task and often renders the question, “where do I even start?”. . Building off of math $ rightarrow$ code, interactive textbooks allow one to see how others have programmed the algorithm. For me, as a self taught programmer, this is a huge value add as it allows me to learn more efficient and better programming principles (assuming the open source code is “efficient”). As well, reading code produced by others allows one to continually develop their skillsets and knowledge within programming—another valuable effect. . These three ingredients is where the bulk of the value add comes. A couple other points worth pointing out are (1) familiarizing yourself with the formalisms pays dividends, and (2) you should read the textbooks where the author resonates with you. Becoming knowledgeable with the formalisms pays dividends as you read other textbooks, research papers, and even venture outside of the field you are in. Their may be some subtleties in notations, but you can recognize this and still manage your way through the technical content you are reading. Secondly, it is often the case I will have three to four books about the same “material”, but I will only “connect” with one or two of the author’s way of describing the concepts. So, don’t be afraid to download several books on the same material (which is easily doable from pdf drive). .",
            "url": "https://gstechschulte.github.io/cached-projects/2021/09/14/Textbooks-Have-Gotten-Good,-Like-Really-Good.html",
            "relUrl": "/2021/09/14/Textbooks-Have-Gotten-Good,-Like-Really-Good.html",
            "date": " • Sep 14, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "No Code, Dependency, and Building Technology",
            "content": "Modernity and Abstraction . ‘Programmers’, loosely speaking, in some form or another have always been developing software to automate tedious and repetitive tasks. Rightly so, as this is one of the tasks computers are designed to perform. As science and technology progresses, and gets more technological, there is a growing seperation between the maker and the user. This is one of the negative externalities of modernism - we enjoy the benefits of a more advanced and technologically adept society, but fewer and fewer people understand the inner workings. Andrej Karpathy has a jokingly short paragraph in his blog on the matter, “A courageous developer has taken the burden of understanding query strings, urls, GET/POST requests, HTTP connections, and so on from you and largely hidden the complexity behind a few lines of code. This is what we are now familiar with and expect”. . Ever since the rise of machine learning and data science, the industry was bound to develop no and or low code products for everything between data cleaning, processing, and annotation, to the implementation of models. This is an example of one of the highest forms of abstraction - you don’t even need to be able to code. Knowledge of the problem at hand and some simple intuition into which model may provide a low validation error is almost all you need. A lower level form of abstraction is through the use of application programming interfaces (APIs) such as scikit-learn, PyTorch, etc. Using these APIs requires more technical skillsets, knowledge of first principles, and a deeper understanding of the problem than the no-code substitutes stated above. Lastly, we have the ‘bare bones’ implementation of algorithms - what most people usually call, “programming things from scratch”. But even then, good luck writing your favorite model without using numpy, scipy, or JAX. . As teams of couragous developers and organizations seek to create products to help make everyday routines a commodity and more productive, it can be harder to learn technical methods from first principles as they have been abstracted away. There’s no need, nor the time, to start and go writing everything from scratch, but having a solid intuition into what is going on under the hood can allow you to uniquely solve complex problems, debug more efficiently, contribute to open source software, etc. . Switching Costs, Dependency and Open Source Goods . At least the two lower levels of abstractions usually rely on open-source software, whereas the no-code alternative is typically (at the moment) proprietary technology in the form of machine learning as a service and or platform. [Edit 14.09.2021: H20.ai, HuggingFace, MakeML, CreateML, Google Cloud AutoML are all open-source services or platforms in the low or no code ML space] Open-source gives you the biggest flexibility that, if the space again changes, you can move things. Otherwise, you find yourself locked into a technology stack the way you were locked in to technologies from the ’80s and ’90s and 2000s. . Being locked into IT components can have serious opportunity costs. For example, switching from Mac to a Windows based PC involves not only the hardware costs of the computer itself, but also involves purchasing of a whole new library of software, and even more importantly, learning how to use a brand new system. When you, or an organization, decides to go the propriety route, these switching costs can be very high, and users may find themselves experiencing lock-in; a situation where the cost of changing to a different system is so high that switching is virtually inconcievable. . Of course, the producers of this hardware / sofware love the fact that you have an inelastic demand curve - a rise in prices won’t affect demand much as switching costs are high. In summary, as machine learning platforms didn’t really exist ~15 years ago, they will more than likely change quite a bit and you are dependent on the producer of continually adapting to industry trends and technological advancements while at the same time, staying flexible to cater to your edge cases when you need it. . Leverage and Building Technology . It would be a full time job to stay present and up to date on everything being released in the space of “machine learning”, but also to be knowledgeable of first princples and have skillsets to use the technologies is in another class of its own. Before AWS, APIs, open source, etc., as an organization or startup, it was likely the question, “we need amazing technical people who can build it all”. Now, with the increase and rise of PaaS, SaaS, open source librarys and tooling, the question shifts from a question of “building”, to a question of “leveraging existing tools”. How long before no / low code gets good enough before we are asking, “why did we not build this with no-code tools?”. . The highest form of leverage for a company is to develop and build out difficult and new technology. No-code, if developed right (and is ideally open-source), can still provide leverage and value-add, but any advantage just becomes table stakes. This is what will distinguish great teams vs. good teams; non-linear returns will continue to be through building out proprietery and difficult technology. .",
            "url": "https://gstechschulte.github.io/cached-projects/2021/08/10/No-Code-Dependency-and-Building-Technology.html",
            "relUrl": "/2021/08/10/No-Code-Dependency-and-Building-Technology.html",
            "date": " • Aug 10, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Intelligent Systems",
            "content": "Infrastructure, Transportation, . | Multiple decisions at scale. . | Uber, AirBnB, Music . | Producer - consumer relationships $ rightarrow$ no advertising . | .",
            "url": "https://gstechschulte.github.io/cached-projects/2021/05/31/Intelligent-Systems.html",
            "relUrl": "/2021/05/31/Intelligent-Systems.html",
            "date": " • May 31, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Economics of Open Source",
            "content": "Why does open-source work? . Highly skilled people devoting their valuable time for little to no monetary benefit. . . . . .",
            "url": "https://gstechschulte.github.io/cached-projects/2021/05/31/Economics-of-Open-Source.html",
            "relUrl": "/2021/05/31/Economics-of-Open-Source.html",
            "date": " • May 31, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://gstechschulte.github.io/cached-projects/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is a home for frequently thought about projects, ideas, and or methodologies related to technology, probabilistic machine learning, and microeconomics. This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://gstechschulte.github.io/cached-projects/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gstechschulte.github.io/cached-projects/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}